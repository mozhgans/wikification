{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing wsd datasets\n",
    "## A bunch of scripts to preprocess and unify the strucutre of several datasets we use in the Wikifier Evaluations\n",
    "\n",
    "Each annotated sentence is converted to the following format:\n",
    "\n",
    "`S = [w1,w2,...,w2]`\n",
    "\n",
    "`M = [[m1,e1], [m2,e2],...[mk, ek]]`\n",
    "\n",
    "where `[wi]`s are the tokens and `[mi,ei]` means `S[mi]` is linked to entity `ei` \n",
    "\n",
    "Example: \n",
    "A sentence such as: `David and Victoria named their children Brooklyn, Romeo, Cruz, and Harper Seven.`\n",
    "\n",
    "is converted to \n",
    "\n",
    "`S=[\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'David and Victoria named their children Brooklyn , Romeo , Cruz , and Harper Seven .'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=[\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"]\n",
    "\" \".join(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing wikimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "import requests\n",
    "import MySQLdb\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "sys.path.insert(0,'../..')\n",
    "from wikisim.wikipedia import *\n",
    "\n",
    "def qualify(r, min_mention=2, min_abmig=2):\n",
    "    \n",
    "    # condition 1\n",
    "    if len(r['opening_annotation']) < min_mention:\n",
    "        #print \"c1 failed\"\n",
    "        return None\n",
    "    \n",
    "    nottop = False\n",
    "    found_min_ambig=False\n",
    "    annotations = json.loads(r['opening_annotation'])\n",
    "    for ann in annotations:\n",
    "        surf = ann['surface_form']\n",
    "        linktitle = ann['url']\n",
    "        wid = title2id(linktitle.encode('utf-8').replace(' ','_'))\n",
    "        # condition 2\n",
    "        if wid is None:\n",
    "            #print \"c2 failed\", linktitle\n",
    "            return None\n",
    "        \n",
    "        allids = anchor2concept(surf)\n",
    "        # condition 3\n",
    "        if not allids:\n",
    "            #print \"c3 failed\"\n",
    "            return None\n",
    "        # condition 4\n",
    "        if len(allids) >= min_abmig:\n",
    "            found_min_ambig = True\n",
    "        allids = sorted(allids, key=lambda k:-k[1])\n",
    "        \n",
    "        # condition 5\n",
    "        #print allids\n",
    "        if allids[0][0] != wid:\n",
    "            nottop = True\n",
    "    if not found_min_ambig:\n",
    "        #print \"c4 failed\"\n",
    "        return None\n",
    "    return nottop                                                                                   \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wanted=36000\n",
    "wanted_nottop=10000\n",
    "\n",
    "reopen()\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='*:*'\n",
    "rows=1000\n",
    "params={'indent':'on', 'wt':'json', 'fl':'title opening_text opening_annotation', 'q':q, \"start\":0,\n",
    "        \"rows\":rows}\n",
    "home = os.path.expanduser(\"~\");\n",
    "output = open(os.path.join(home,'backup/datasets/ner/wiki.%s.json'%(wanted, )), 'w')\n",
    "\n",
    "start=0\n",
    "count=0\n",
    "count_nottop=0\n",
    "enough=False\n",
    "while True:\n",
    "    params[\"start\"] = start\n",
    "    start += rows\n",
    "    r = requests.get(qstr, params=params)\n",
    "    #print r.json()['response']['docs']\n",
    "    for d in r.json()['response']['docs']:\n",
    "        #print d[\"id\"]; enough=True; break\n",
    "        if not (count < wanted or count_nottop < wanted_nottop):\n",
    "            enough = True\n",
    "            break\n",
    "        qd = qualify(d) \n",
    "        if qd is None:\n",
    "            continue\n",
    "            \n",
    "        if count_nottop < wanted_nottop and qd :\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            count_nottop += 1\n",
    "            count += 1\n",
    "            if ((count) % int(wanted/10) ==0) or (count_nottop) % int(wanted_nottop/10) ==0:\n",
    "                print \"count: \", count, \", count_nottop: \", count_nottop\n",
    "            continue\n",
    "        if count < wanted:\n",
    "            count +=1\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            if (count) % int(wanted/10) ==0:   \n",
    "                print \"count: \", count\n",
    "            \n",
    "    #break\n",
    "    if enough:\n",
    "        break\n",
    "print 'done'        \n",
    "output.close()    \n",
    "#r=json.loads(r['opening_annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting json to input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, json\n",
    "import string\n",
    "home = os.path.expanduser(\"~\");\n",
    "wanted=36000\n",
    "jsname = os.path.join(home,'backup/datasets/ner/wiki.%s.json'%(wanted,))\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.%s.json'%(wanted,))\n",
    "\n",
    "\n",
    "    \n",
    "def splittext(text,url):\n",
    "    start=0\n",
    "    termindex=0\n",
    "    t=[]\n",
    "    mentions=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for u in url:\n",
    "        seg = text[start:u['from']]\n",
    "        t += seg.strip().split()\n",
    "        mentions.append([len(t),u['url']])\n",
    "        t+=[\" \".join(text[u['from']:u['to']].split())]\n",
    "        start = u['to']\n",
    "        \n",
    "    t += text[start:].strip().split()\n",
    "    return t, mentions\n",
    "\n",
    "with open(jsname) as jf, open(outjsname,'w') as outjs:\n",
    "    for line in jf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        js = json.loads(line)\n",
    "        text = js['opening_text'] \n",
    "        url = json.loads(js['opening_annotation'])\n",
    "        t, m = splittext(text, url)\n",
    "        outjs.write((json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False)+'\\n').encode('utf-8'))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing KORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/KORE50/AIDA.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/kore.json')\n",
    "url_col = 3\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            line = line.decode('unicode-escape')\n",
    "        except:\n",
    "            pass            \n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "        if len(w) == 1:\n",
    "            t.append(w[0])\n",
    "            continue\n",
    "            \n",
    "        if w[1] == 'B':\n",
    "            if title2id(w[3]) is None:\n",
    "                print w[3], \" not found\"\n",
    "            if w[url_col] != '--NME--' and (title2id(w[3]) is not None):\n",
    "                m.append((len(t), w[3]))\n",
    "            t.append(w[2])\n",
    "            \n",
    "        if w[1] == 'I':\n",
    "            continue\n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=\"Rudi_V\\u00f6ller\"\n",
    "print x.decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "import re\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/Aida01/aida-yago2-dataset/AIDA-YAGO2-annotations.tsv')\n",
    "#infname = os.path.join(home,'backup/datasets/ner/aida-shalam.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/aida.json')\n",
    "#outjsname = os.path.join(home,'backup/datasets/ner/aida-shalam.json')\n",
    "# set it to 3 for AIDA, \n",
    "url_col = 4\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        #line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "            \n",
    "        if w[1] == '--NME--':\n",
    "            continue\n",
    "        title = w[2]\n",
    "        r = re.match(pattern, title)\n",
    "        title = r.group(1)\n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        anchors = id2anchor(rwd)\n",
    "        if not anchors:\n",
    "            print \"no anchor for: \", title\n",
    "            continue\n",
    "                            \n",
    "        c=max(anchors, key=lambda k:k[1])[0]    \n",
    "        m.append((len(t), title))\n",
    "        t.append(c.decode('utf-8'))\n",
    "            \n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                \n",
    "print \"done\"        \n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing MASNBC/AQUAINT (Ignoring the text (for WSD-only evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../../wikisim/')\n",
    "from wikipedia import *\n",
    "home = expanduser(\"~\");\n",
    "mypath = join(home, 'backup/datasets/WikificationACL2011Data/AQUAINT/Problems/')\n",
    "outjsname = join(home,'backup/datasets/ner/aquaint.json')\n",
    "outjs=open(outjsname,'w')\n",
    "dsnames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for ds in dsnames:\n",
    "    \n",
    "    with open(join(mypath, ds)) as f:\n",
    "        filestr=f.read()\n",
    "    #print filestr\n",
    "    filestr=filestr.decode(encoding='cp1252').replace('&','&amp;').encode(encoding='utf-8')\n",
    "    root=ET.fromstring(filestr)\n",
    "    \n",
    "    \n",
    "    t=[]\n",
    "    m=[]\n",
    "    i=0\n",
    "    for child in root.findall('ReferenceInstance'):\n",
    "        anchor=child.find('SurfaceForm').text.strip()\n",
    "        url = child.find('ChosenAnnotation').text.strip()\n",
    "        if url == '*null*':\n",
    "            continue\n",
    "        if url[:29] !='http://en.wikipedia.org/wiki/':\n",
    "            print url[:29]\n",
    "            raise Exception('bad format')\n",
    "        url=url[29:]\n",
    "        if title2id(url) is None:\n",
    "            print ds,':',url, \" not found\"\n",
    "            continue\n",
    "        \n",
    "        t.append(anchor)\n",
    "        m.append([i,url])\n",
    "        i +=1\n",
    "        \n",
    "    outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "outjs.close()\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing MSNBC/AQUAINT with Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus3683270.txt\n",
      "TvN16442342.txt\n",
      "Tec16451635.txt\n",
      "Spo16417540.txt\n",
      "Tec16454435.txt\n",
      "Ent16453733.txt\n",
      "Tra16444229.txt\n",
      "Tra16454203.txt\n",
      "Pol16447720.txt\n",
      "Wor16447201.txt\n",
      "Hea16384904.txt\n",
      "Wor13259309.txt\n",
      "Bus16451112.txt\n",
      "Spo16455207.txt\n",
      "Hea16451212.txt\n",
      "Ent16444023.txt\n",
      "USN16443053.txt\n",
      "USN16444287.txt\n",
      "TvN16442287.txt\n",
      "Pol16452612.txt\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../../wikisim/')\n",
    "from wikipedia import *\n",
    "home = expanduser(\"~\");\n",
    "\n",
    "\n",
    "#ds_dir = 'AQUAINT'\n",
    "ds_dir = 'MSNBC'\n",
    "p_path = join('../../datasets/ner/source/WikificationACL2011Data', ds_dir )\n",
    "\n",
    "out_jsname = join('../../datasets/ner/', ds_dir.lower()+'.txt.json')\n",
    "\n",
    "text_path = join(p_path,'RawTexts' )\n",
    "problem_path = join(p_path,'Problems' )\n",
    "\n",
    "outjs=open(out_jsname,'w')\n",
    "def replace_bad_chars(s):\n",
    "    return s.replace(\"\\x85\", '.').replace(\"\\x91\",\"'\").replace(\"\\x92\",\"'\").replace(\"\\x93\",'\"')\\\n",
    "            .replace(\"\\x94\", '\"').replace(\"\\x96\", '-').replace(\"\\x97\", '-').replace(\"\\xfc\",'x')\n",
    "\n",
    "dsnames = [f for f in listdir(problem_path) if isfile(join(problem_path, f)) and (re.match(r'.*\\.htm$', f) or re.match(r'.*\\.txt$', f))]\n",
    "def splittext(text,url):\n",
    "    start=0\n",
    "    termindex=0\n",
    "    t=[]\n",
    "    mentions=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for u in url:\n",
    "        seg = text[start:u['from']]\n",
    "        t += seg.strip().split()\n",
    "        mentions.append([len(t),u['url']])\n",
    "        t+=[\" \".join(text[u['from']:u['to']].split())]\n",
    "        start = u['to']\n",
    "        \n",
    "    t += text[start:].strip().split()\n",
    "    return t, mentions\n",
    "\n",
    "def get_annotation(filename):\n",
    "    with open(join(problem_path, filename)) as f:\n",
    "        annotstr=f.read()\n",
    "    #print filestr\n",
    "    #annotstr=annotstr.decode(encoding='cp1252').replace('&','&amp;').encode(encoding='utf-8')\n",
    "    \n",
    "    annotstr=replace_bad_chars(annotstr).replace('&','&amp;').strip()\n",
    "    root=ET.fromstring(annotstr)\n",
    "    \n",
    "    \n",
    "    t=[]\n",
    "    for child in root.findall('ReferenceInstance'):\n",
    "        anchor=child.find('SurfaceForm').text.strip()\n",
    "        url = child.find('ChosenAnnotation').text.strip()\n",
    "        if url == '*null*':\n",
    "            continue\n",
    "        if url[:29] !='http://en.wikipedia.org/wiki/':\n",
    "            print url[:29]\n",
    "            raise Exception('bad format')\n",
    "        url=url[29:]\n",
    "        if title2id(url) is None:\n",
    "            #print ds,':',url, \" not found\"\n",
    "            continue\n",
    "        ufrom = int(child.find('Offset').text.strip()) \n",
    "        uto = ufrom + int(child.find('Length').text.strip())\n",
    "        t.append({'from':ufrom, 'to':uto, \n",
    "                'url':url, 'anchor': anchor})\n",
    "    return t\n",
    "\n",
    "for ds in dsnames:\n",
    "    print ds\n",
    "    url = get_annotation(ds)\n",
    "    with open(join(text_path, ds)) as f:\n",
    "        text=f.read()\n",
    "    text=replace_bad_chars(text).decode('utf-8')\n",
    "    \n",
    "    #break\n",
    "    t, m = splittext(text, url)\n",
    "    outjs.write((json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False)+'\\n').encode('utf-8'))\n",
    "    #print filestr\n",
    "\n",
    "        \n",
    "outjs.close()\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open ('/home/sajadi/backup/datasets/ner/MSNBC.txt.json') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.decode('utf-8')\n",
    "        jline = json.loads(line)\n",
    "        for ann in jline['mentions']:\n",
    "            print jline['text'][ann[0]],'-->', ann[1]\n",
    "        count +=1\n",
    "        if count >= 10:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
