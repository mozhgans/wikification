{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train-test for senseembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materialization of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting materialize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile materialize.py \n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from multiprocessing import Pool, Process, Manager \n",
    "import functools\n",
    "import thread\n",
    "from requests.packages.urllib3 import Retry\n",
    "sys.path.insert(0,'../..')\n",
    "home = os.path.expanduser(\"~\");\n",
    "from wikisim.wikipedia import *\n",
    "\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "process_no=25\n",
    "tr_percent=0.8\n",
    "\n",
    "down_sample = True\n",
    "max_anchor = 100\n",
    "skip_line=-1\n",
    "\n",
    "random.seed(3)\n",
    "written_sofar=0\n",
    "example_per_anchor=10\n",
    "\n",
    "session = requests.Session()\n",
    "http_retries = Retry(total=20,\n",
    "                backoff_factor=.1)\n",
    "http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "session.mount('http://localhost:8983/solr', http)\n",
    "\n",
    "def solr_escape(s):\n",
    "    return re.sub(r'''['\"\\\\]''', r'\\\\\\g<0>', s)\n",
    "\n",
    "def get_context(anchor, eid):\n",
    "    \n",
    "    params={'wt':'json', 'rows':'50000'}\n",
    "    anchor = solr_escape(anchor)\n",
    "    \n",
    "    q='anchor:\"%s\" AND entityid:%s' % (anchor, eid)\n",
    "    params['q']=q\n",
    "    \n",
    "#     session = requests.Session()\n",
    "#     http_retries = Retry(total=20,\n",
    "#                     backoff_factor=.1)\n",
    "#     http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "#     session.mount('http://localhost:8983/solr', http)\n",
    "    \n",
    "    r = session.get(qstr, params=params).json()\n",
    "    if 'response' not in r: \n",
    "        print \"[terminating]\\t%s\",(str(r),)\n",
    "        sys.stdout.flush()\n",
    "        os._exit(0)\n",
    "        \n",
    "    if not r:\n",
    "        return []\n",
    "    return r['response']['docs']\n",
    "\n",
    "def loadanchors(min_count=5):\n",
    "    rows = load_table('anchors')\n",
    "    anchors = defaultdict(list)\n",
    "    for r in rows:\n",
    "        if r[2] >= min_count:\n",
    "            anchors[r[0]].append((r[1], r[2]))        \n",
    "    return anchors.items()\n",
    "\n",
    "\n",
    "def mater_anchor((a,l), trq, tsq, lgq):\n",
    "    global written_sofar\n",
    "    if down_sample and written_sofar >=max_anchor:\n",
    "        return\n",
    "    if (not a) or len(l)<2:\n",
    "        lgq.put( \"[Error]\\tanchor_empty_or_not_ambig\\t%s]\" % json.dumps({\"anchor\": a, \"length\": l}))\n",
    "        return\n",
    "    #print '(wid,n) = ', (a,l)\n",
    "    for i in range(len(l)):\n",
    "        (wid,f) = l[i]\n",
    "        neg = l[:i]+l[i+1:]\n",
    "        #neg = [nid for (nid, _) in neg]\n",
    "        contexts = get_context(a,wid)        \n",
    "        n=len(contexts)\n",
    "        \n",
    "        random.shuffle(contexts)\n",
    "        \n",
    "        if down_sample:\n",
    "            contexts = contexts[:example_per_anchor]\n",
    "            n=len(contexts)\n",
    "                        \n",
    "        if not contexts:\n",
    "            lgq.put(\"[Error]\\tcontext_empty\\t%s\" % json.dumps({\"wid\": wid, \"frq\": f}))\n",
    "            continue\n",
    "        # now we have a     \n",
    "        cutpoint=int(math.ceil(tr_percent*n))\n",
    "        if skip_line==-1:\n",
    "            train = contexts[:cutpoint]\n",
    "            test = contexts[cutpoint:]\n",
    "        else:\n",
    "            train = [c for c in contexts if skip_line not in c['paragraph_no']]\n",
    "            test = [c for c in contexts if skip_line in c['paragraph_no']]\n",
    "            \n",
    "        lgq.put (\"[success]\\t%s\" % json.dumps({\"anchor\": a,\"wid\": wid, \"freq\": f, \"context_length\": n,\n",
    "                                            \"train_size\":len(train), \"test_size\":len(test)}))\n",
    "        \n",
    "        mater_sample(train, neg, trq)    \n",
    "        mater_sample(test, neg, tsq)    \n",
    "        if down_sample:\n",
    "            written_sofar += 1\n",
    "def mater_sample(context, neg, q):\n",
    "    for c in context:\n",
    "        c.pop('id', None)\n",
    "        c.pop('_version_', None)\n",
    "        q.put(json.dumps({\"context\":c, \"neg\": neg, \"freq\": len(c)},ensure_ascii=False).encode('utf-8'))\n",
    "        \n",
    "def worker(fname, q):\n",
    "    w = open(fname,'w')\n",
    "    print \"[Writer started]\"\n",
    "    sys.stdout.flush()\n",
    "    while True:\n",
    "        s = q.get()\n",
    "        if s=='kill':\n",
    "            print \"[Writer worker closing]\"\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "        w.write(s+\"\\n\")\n",
    "    w.close()    \n",
    "\n",
    "    \n",
    "startTime = time.time()\n",
    "anchors = loadanchors()    \n",
    "print '[anchors loaded to memory]'    \n",
    "print time.time()-startTime\n",
    "sys.stdout.flush()\n",
    "        \n",
    "startTime = time.time()\n",
    "\n",
    "manager= Manager()\n",
    "\n",
    "extension='%s.%s.json'%(down_sample, skip_line)\n",
    "if down_sample:\n",
    "    extension=\"%s.%s\"%(max_anchor, extension)\n",
    "    \n",
    "train_name = os.path.join(home,'backup/datasets/cmod/train.%s'%(extension))\n",
    "test_name = os.path.join(home,'backup/datasets/cmod/test.%s'%(extension))\n",
    "log_name = os.path.join(home,'backup/datasets/cmod/log.%s'%(extension))\n",
    "    \n",
    "train_q = manager.Queue()\n",
    "test_q = manager.Queue()\n",
    "log_q = manager.Queue()\n",
    "\n",
    "\n",
    "train_proc = Process(target=worker, args=(train_name, train_q))\n",
    "train_proc.start()   \n",
    "        \n",
    "test_proc = Process(target=worker, args=(test_name, test_q))\n",
    "test_proc.start()   \n",
    "\n",
    "log_proc = Process(target=worker, args=(log_name, log_q))\n",
    "log_proc.start()   \n",
    "\n",
    "\n",
    "#pool = Pool(process_no) \n",
    "#pool.map(functools.partial(mater_anchor, trq=train_q, tsq=test_q ), anchors)\n",
    "map(functools.partial(mater_anchor, trq=train_q, tsq=test_q, lgq=log_q   ), anchors)\n",
    "\n",
    "train_q.put('kill')    \n",
    "test_q.put('kill')\n",
    "log_q.put('kill')\n",
    "\n",
    "train_proc.join()\n",
    "test_proc.join()\n",
    "log_proc.join()\n",
    "\n",
    "print 'Done'    \n",
    "print time.time()-startTime\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# integizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import collections\n",
    "import json\n",
    "\n",
    "def build_vocab(words, min_count=5):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend([ (w,c) for w,c in collections.Counter(words).items()])\n",
    "    vocab = dict()\n",
    "    for word, c in count:\n",
    "        if c >= min_count:\n",
    "            vocab[word] = len(vocab)\n",
    "    return count, vocab\n",
    "\n",
    "def getwords(*filenames):\n",
    "    words=[]\n",
    "    for filename in filenames:\n",
    "        with open(filename) as infile:\n",
    "            for line in infile:\n",
    "                ex = json.loads(line.decode('utf-8').strip())\n",
    "                words += [str(n[0]) for n in ex[\"neg\"]]\n",
    "                if \"left\" in ex[\"context\"]:\n",
    "                    words += ex[\"context\"][\"left\"].split()\n",
    "                if \"right\" in ex[\"context\"]:\n",
    "                    words += ex[\"context\"][\"right\"].split()\n",
    "                words .append(ex[\"context\"][\"entityid\"])\n",
    "    return words\n",
    "        \n",
    "def integize(infile_name, outfile_name, vocab):\n",
    "    with open(infile_name) as infile, open(outfile_name, 'w' ) as outfile:\n",
    "        for line in infile:\n",
    "            ex = json.loads(line.decode('utf-8').strip())\n",
    "            \n",
    "            neg =  [[vocab[str(n[0])],n[1]] for n in ex[\"neg\"] if str(n[0]) in vocab]                \n",
    "            if not neg or ex[\"context\"][\"entityid\"] not in vocab:\n",
    "                continue\n",
    "                \n",
    "            entityid  = vocab[ex[\"context\"][\"entityid\"]]\n",
    "                \n",
    "            if \"left\" in ex[\"context\"]:\n",
    "                left = [vocab[w] for w in ex[\"context\"][\"left\"].split() if w in vocab]\n",
    "            if \"right\" in ex[\"context\"]:\n",
    "                right = [vocab[w] for w in ex[\"context\"][\"right\"].split() if w in vocab]\n",
    "            \n",
    "            \n",
    "            ex_id = {\"neg\": neg, \n",
    "                     \"context\": { \"left\": left, \"entityid\" : entityid, \n",
    "                                 \"right\": right, \"freq\":ex[\"freq\"] },                     \n",
    "                    }\n",
    "            outfile.write(json.dumps(ex_id, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#%%writefile integize.py\n",
    "import os\n",
    "from utils import *\n",
    "home = os.path.expanduser(\"~\")\n",
    "filepattern='10000.True.0'\n",
    "train_name_w = os.path.join(home, 'backup/datasets/cmod/train.%s.json'%(filepattern,))\n",
    "train_name = os.path.join(home, 'backup/datasets/cmod/train.id.%s.json'%(filepattern,))\n",
    "test_name_w = os.path.join(home, 'backup/datasets/cmod/test.%s.json'%(filepattern,))\n",
    "test_name = os.path.join(home, 'backup/datasets/cmod/test.id.%s.json'%(filepattern,))\n",
    "\n",
    "words = getwords(train_name_w, test_name_w)\n",
    "count, vocab = build_vocab(words, min_count=5)\n",
    "with open(os.path.join(home, 'backup/datasets/cmod/vocab.%s.tsv'%(filepattern,)), 'w') as out:\n",
    "    out.write(json.dumps({\"orig_size\": len(count), \"size\": len(vocab)})+'\\n')\n",
    "    out.write(json.dumps(count, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "    out.write(json.dumps(vocab, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "\n",
    "\n",
    "integize(train_name_w, train_name, vocab)\n",
    "integize(test_name_w, test_name, vocab)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for word2vec\n",
    "## modifying word2vec/word2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile replace_surface.py\n",
    "import re\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "from HTMLParser import HTMLParser\n",
    "import time\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from memapi import memwiki as wiki\n",
    "\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "def title2id(title):\n",
    "    if not title:\n",
    "        return \"NA1\"\n",
    "    wid = wiki.title2id(title)\n",
    "    if wid is None:\n",
    "        title=title[0].upper()+title[1:]    \n",
    "        wid = wiki.title2id(title)\n",
    "    if wid is None:\n",
    "        return \"NA2\"\n",
    "    return str(wid)\n",
    "    \n",
    "\n",
    "def url2id(antext, url):\n",
    "    hp = HTMLParser()\n",
    "    \n",
    "    url=url.encode('utf-8')\n",
    "    url =  urllib.unquote(url)\n",
    "    url = url.decode('utf-8')\n",
    "\n",
    "    url=hp.unescape(url)\n",
    "    url=hp.unescape(url)\n",
    "    url=url.replace(u\"\\xA0\",\" \")\n",
    "    x = url.find(\"#\")\n",
    "    if x!=-1:\n",
    "        url=url[:x]\n",
    "    return \"id_\"+title2id(url)\n",
    "    \n",
    "    \n",
    "def replacelinks(text):\n",
    "    \n",
    "    annotations = []\n",
    "    deltaStringLength = 0\n",
    "    hrefreg=r'<a href=\"([^\"]+)\">([^>]+)</a>'\n",
    "    \n",
    "    text = re.sub(hrefreg, lambda m:url2id(m.group(2), m.group(1)), text)  \n",
    "    return text\n",
    "\n",
    "\n",
    "def process():\n",
    "    hp = HTMLParser()\n",
    "    rstart=r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">'\n",
    "    rend=r'</doc>'\n",
    "    \n",
    "    line_no=-1;\n",
    "    for line in fileinput.readlines():\n",
    "        line = line.decode('utf-8').strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        ms = re.match(rstart, line)\n",
    "        if ms is not None:\n",
    "            wid=ms.group(1)\n",
    "            wtitle=hp.unescape(ms.group(3)).replace(u\"\\xA0\",\" \")\n",
    "            line_no=0\n",
    "            #print 'id_'+title2id(wtitle)\n",
    "            continue\n",
    "        if line_no ==0:\n",
    "            line_no=1\n",
    "            continue\n",
    "        if re.match(rend,line):\n",
    "            print \"\\n\"\n",
    "            continue    \n",
    "\n",
    "        text = replacelinks(line).encode('utf-8')\n",
    "        print text\n",
    "        continue\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    #startTime = time.time()\n",
    "    wiki.load_tables()\n",
    "    #print 'wiki loaded to memory'    \n",
    "    #print time.time()-startTime\n",
    "    #sys.stdout.flush()\n",
    "    \n",
    "    process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qstr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "process_no=25\n",
    "tr_percent=0.8\n",
    "\n",
    "down_sample = True\n",
    "max_anchor = 100\n",
    "skip_line=-1\n",
    "\n",
    "random.seed(3)\n",
    "written_sofar=0\n",
    "example_per_anchor=10\n",
    "\n",
    "session = requests.Session()\n",
    "http_retries = Retry(total=20,\n",
    "                backoff_factor=.1)\n",
    "http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "session.mount('http://localhost:8983/solr', http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
