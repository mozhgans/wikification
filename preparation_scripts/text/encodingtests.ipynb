{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big confusion that  started this:\n",
    "When I read from the text file (utf-8 encode) and do title2id, works fine! When I do the same thing from json (and again utf-8) encoded, shit happens! \n",
    "\n",
    "### tldr;\n",
    "if you read title from normal text file, it's already binary and you can query, if you have decoded, you should encode again. \n",
    "If you read from json, you always should encode the title\n",
    "You were lucky that in the `annonatate.py`, you used id and not title!\n",
    "\n",
    "### A bit history\n",
    "\n",
    "You encode a unicode string, meaning that, you convert it to byte representation\n",
    "\n",
    "Unicode:\n",
    "4 bytes, max 0x10FFFF\n",
    "normally encoding, meaning that putting the actuall codepoint, is a waste of space\n",
    "\n",
    "utf-8 is compact and blah blah!\n",
    "\n",
    "Normally the files contain the encoded thing, and if what you do is merely byte operation, no need to decode and encode. The following piece of code works (meaning that the article is found, and the find is readable by vim) because the default string in python is ascii and not unicode, matches the file, and the database (which is also binary) (I am using ascii and binary interchangably, not quite sure though)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: I did this when I was a bit ignorant! The result should be obvious\n",
    "\n",
    "This shows that if I don't touch anything, everythig is fine, Anarchism is an article with unicode text (alpha...), the second title is problamtic, and the third one is even worse (sometimes breaks when you want to title2id it). \n",
    "\n",
    "in Anarchism, there are multiple problomatic words that can be traced, such as the sentence with \"verbal infinitive suffix -ίζειν, -izein\" , surprisingly \"people’s ‘mental enslavement’\" is encoded strangely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../cgi-bin/')\n",
    "from wikipedia import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">'\n",
    "\n",
    "import re\n",
    "import json\n",
    "query = \"Anarchism\"\n",
    "query = \"Asociación Alumni\"\n",
    "# query = \"Atanasoff–Berry computer\"\n",
    "# query = \"Bunge &amp; Born\"\n",
    "\n",
    "output1=open('out1', 'w')\n",
    "output2=open('out2', 'w')\n",
    "found = False\n",
    "text=\"\"\n",
    "with open('/users/grad/sajadi/backup/wikipedia/20160305/texts/xmltexts/wiki_00') as f:\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        r = re.match(r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">',line)\n",
    "        if r is not None:\n",
    "            title=r.group(3)\n",
    "            if title == query:\n",
    "                found=True\n",
    "                continue\n",
    "        if not found:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if re.match('</doc>', line):\n",
    "            break\n",
    "            \n",
    "        text += line + '\\n' \n",
    "output1.write(title+\"\\n\"+text.strip())        \n",
    "output1.close()\n",
    "output2.write(json.dumps({\"title\":title, \"text\": text.strip()}, ensure_ascii=False))\n",
    "output2.close()\n",
    "\n",
    "wid=title2id(title.replace(\" \",\"_\"))\n",
    "if wid is not None:\n",
    "    print \"Passed, id: \", wid\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code I have written (tojson), I have  decoded/encoded. It made some part of the code complicated (encode before unquoting, and decode after). I could avoid that, skipping those lines and changing one line of the code:\n",
    "\n",
    "`url=url.replace(u\"\\xA0\",\" \")`\n",
    "\n",
    "by\n",
    "\n",
    "`nbsp=hp.unescape(\"&nbsp;\").encode('utf-8')\n",
    "url=url.replace(nbsp,\" \")`\n",
    "\n",
    "Buyt which one is better? Probably decoding/encoding. \n",
    "\n",
    "I wrote a code to see if it changes anything, obviously not when I think about it now, but then I was confused!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">'\n",
    "from wikipedia import *\n",
    "import re\n",
    "import json\n",
    "query = \"Anarchism\"\n",
    "query = \"Asociación Alumni\".decode('utf-8')\n",
    "query = \"Atanasoff–Berry computer\".decode('utf-8')\n",
    "query = \"Bunge &amp; Born\".decode('utf-8')\n",
    "\n",
    "output3=open('out3', 'w')\n",
    "output4=open('out4', 'w')\n",
    "found = False\n",
    "text=\"\"\n",
    "with open('/users/grad/sajadi/backup/wikipedia/20160305/texts/xmltexts/wiki_00') as f:\n",
    "    for line in f:\n",
    "        line=line.decode('utf-8').strip()\n",
    "        r = re.match(r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">',line)\n",
    "        if r is not None:\n",
    "            title=r.group(3)\n",
    "            if title == query:\n",
    "                found=True\n",
    "                continue\n",
    "        if not found:\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if re.match('</doc>', line):\n",
    "            break\n",
    "            \n",
    "        text += line + '\\n' \n",
    "output3.write(json.dumps({\"title\":title, \"text\": text.strip()}, ensure_ascii=False).encode('utf-8'))\n",
    "output3.close()\n",
    "output4.write(json.dumps({\"title\":title.encode('utf-8'), \"text\": text.encode('utf-8').strip()}, ensure_ascii=False))\n",
    "output4.close()\n",
    "\n",
    "#wid=title2id(title.replace(\" \",\"_\"))\n",
    "if wid is not None:\n",
    "    print \"Passed, id: \", wid\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so output 2 to 4 are the same, but how about 1 and 2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('out1') as f:\n",
    "    line1 = f.readline().strip()\n",
    "    title1 = line1\n",
    "    print title1\n",
    "    \n",
    "with open('out2') as f:\n",
    "    line2 = f.readline().strip()\n",
    "    \n",
    "r = re.search(r'\"title\": \"([^\"]*)\"', line2)\n",
    "title2 = r.group(1)\n",
    "print title2\n",
    "\n",
    "print title1==title2    \n",
    "\n",
    "title3 = json.loads(line2, encoding='utf-8')['title']\n",
    "print title3\n",
    "print title1 == title3\n",
    "\n",
    "title4 = title3.encode('utf-8')\n",
    "print title1 == title4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example kinda proves the data is totally the same in both of the files, and it's the json.loads that converts it to unicode by default, so I have to encode it if I want to send it to the database. Problem solved! \n",
    "\n",
    "** That encoding='utf-8' doens't do anything apparently! Neighter the decoding of the line. But I guess it's always better to decode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out2') as f:\n",
    "    line1 = f.readline().strip()\n",
    "line2=line1.decode('utf-8')\n",
    "print line1==line2\n",
    "print json.loads(line1) == json.loads(line2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about Solr?? Should I send the binary? Does it save encoded or decoded? Yes, look down (basically don't worry about it)\n",
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "\n",
    "query = (\"Anarchism\", \"Asociación_Alumni\", \"Atanasoff–Berry_computer\", \"Bunge_&_Born\")\n",
    "wids = [title2id(q) for q in query]\n",
    "titles = [id2title(wid) for wid in wids] \n",
    "for  title in titles:\n",
    "    q='title:'+title\n",
    "    params={'indent':'on', 'wt':'json',\"rows\":1, \"fl\":\"id title\", 'q':q}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    r =  r.json()['response']['docs'][0]\n",
    "    print r['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print r.js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Open problem!!\n",
    "Solrtagger, like java, uses utf-16, python utf-8. Can't get around it\n",
    "Forexample, consider the first following sentence, T1, or the second, the opening text of an article, T2\n",
    "The offset is shifter after the problematic character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a5723a7bd197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#print  text[tag[1]:tag[3]], tag[5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "T1=u'I ate 𩬅 in China and blah blah'\n",
    "\n",
    "addr='http://localhost:8983/solr/enwiki20160305/select?indent=on&q=id:%22245791%22&wt=json'\n",
    "r = requests.get(addr).json()['response']['docs']\n",
    "T2=r[0]['opening_text']\n",
    "\n",
    "text = T1\n",
    "addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on', 'matchText':'true'}\n",
    "#text=solr_escape(text)\n",
    "r = requests.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "\n",
    "for tag in r.json()['tags']:\n",
    "    #print  text[tag[1]:tag[3]], tag[5]\n",
    "    assert text[tag[1]:tag[3]] == tag[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
