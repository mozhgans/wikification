{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing A New Wiki Dump \n",
    "\n",
    "**Performing these steps can be tricky and there are better option [see README](../../README.md#hosting)**\n",
    "\n",
    "* **Make sure you have alreay followed the first two steps of the setup explained in [README.md](../../README.md#hosting)**\n",
    "* ** You can find most of the intermediate files in our website reporitory (the permament address can be found [here](../../README.md#api)) **\n",
    "* Note that all of the paths are hard-coded and you need to modify them, consistently, and carefully!\n",
    "\n",
    "\n",
    "\n",
    "But if you want to start frome scratch and import a new Wikipedia version, you should start from here \n",
    "to prepapre the graph structure. This is enough for calculating *concept embedding* and *semantic relatedness*. \n",
    "However, for the text databases, necessary for Word-sense disambiguation, you should proceed [here](../text/prepare_annonated_indexed_wiki.ipynb). \n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "\n",
    "The first step is to download the wikipedia database dumps and import them to mysql. We do a preprocessing on the sql dumps for mainly three reasons:\n",
    "\n",
    "* The tables are huge, containing many column and rows we do not use. Removing the unnessary information, that includes unused columns (such as time stamps, viewed count of the pages or categories) and all the information about talk pages, media files or user draft pages, can dramatically decreas the size of the tables.\n",
    "\n",
    "* Forming **synonym Rings**. We extend the concept of synonym ring to Wikipedia (similar to what is called synset in Wordnet). In Wikipedia, redirection stands for equivallency, for example Car --> Automobile. But it's not always this easy and you can find all sorts of weired redirection, like:\n",
    "\n",
    "![](../../resrc/sr.jpg)\n",
    "\n",
    "   We iterate through redirectins and remove cycles, dangling redirections and also all the chains. This process forms clusters of redirections around main pages. Then we go through all other tables (pagelinks and  category links) and replace any redirected page by its main article, the result would be much more neated, and makes the rest of the process faster.\n",
    "\n",
    "\n",
    "* We remove garbage, links to non existing pages, self links, mismatching namespaces, and many other incosistencies that you can find the details in the source code).\n",
    "\n",
    "* We apply some strategic changes, like instead of source id --> destination title format of the pagelinks, we use source id --> dest id, which is faster and preferrabel for out case. \n",
    "\n",
    "To complete this step, download and run the parser (written in Java) that prunes these files. You can run the following cells, but due to a known bug with ipython, you can't see bash progress messages untill the job is finished. So a better option would be simply running the scripts from bash and skipping the remaining of this section. In this case, running each cell create a script in the [preparation_scripts/] directory with the name indicated as the argument of `writefile` at the begining of the cell. If you want to run the cell directly, ** comment the first line and uncomment the second line**.\n",
    "\n",
    "## Step 0. Installing MySQL (we use MariaDB)\n",
    "Install and run the server, you may want to increase the variables (you can check our .my.cnf)\n",
    "\n",
    "**All of the paths are hard-coded and arbitrary (and relative to the current directory), you need to change them consistently.**\n",
    "\n",
    "## Step 1. Downloading\n",
    "\n",
    "Download the following files and decompress them (we assume the path to be `../../../data/enwiki20160305/original`):\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20160305/enwiki-latest-page.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20160305/enwiki-latest-pagelinks.sql.gz \n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20160305/enwiki-latest-redirect.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20160305/enwiki-latest-category.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/20160305/enwiki-latest-categorylinks.sql.gz\n",
    "\n",
    "### Or\n",
    "**Use the the prepared following script that download wikipedia dumps and decompress them**\n",
    "\n",
    "\n",
    "`bash download.sh ../../../data/enwiki20160305/original`\n",
    "\n",
    "`bash decompress.sh ../../../data/enwiki20160305/original`\n",
    "\n",
    "\n",
    "## Step 2. Parsing Database dumps\n",
    "\n",
    "The following java file  does the preprosseing (parsing) wikipedia dumps and creates the processed tables (ending in `main.sql`) and several log files of the errors\n",
    "\n",
    "*Note*: you might need to recompile (`javac ProcessSQLDumps.java`) \n",
    "\n",
    "run by\n",
    "\n",
    "`java ProcessSQLDumps ../../../data/enwiki20160305/original`\n",
    "\n",
    "\n",
    "`mkdir ../../../data/enwiki20160305/edited`\n",
    "\n",
    "`mv ../../../data/enwiki20160305/original/*.tsv ../../../data/enwiki20160305/edited/`\n",
    "\n",
    "### Step 3. Preparing mysql\n",
    "Running the folling cell will set some variable in mysql for maximum performance (if you have enoguh physical memory). Replace \\$1 and \\$2 with the actuall user and password of the user, or run the script as:\n",
    "\n",
    "`bash setupmysql.sh <user> <pass>`\n",
    "\n",
    "\n",
    "### Step 4. Actuall Importing\n",
    "\n",
    "```mysql -u <user> -p<pass> -e 'CREATE SCHEMA `enwiki20160305` DEFAULT CHARACTER SET binary;'```\n",
    "\n",
    "`./importall  ../../../data/enwiki20160305/edited last <user> <pass>`\n",
    "\n",
    "This might take several hours \n",
    "\n",
    "### If you want to setup the Apache Solr server, continue [here](../text/prepare_annonated_indexed_wiki.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
