{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "** Local and Global Algorithms ... **\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne \n",
    "\n",
    "Downloadable from :\n",
    "http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "\n",
    "**Spotlight**\n",
    "two datasets, a wiki selection\n",
    "35 paragraphs from New York times\n",
    "There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me:\n",
    "Wiki and tweet, \n",
    "available, but looks old!\n",
    "http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "**AIDA**\n",
    "\n",
    "https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/\n",
    "AIDA CoNLL-YAGO Dataset: Hnad create from Conll\n",
    "AIDA-EE Dataset: Again hand done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.py \n",
    "\"\"\" Config file.\n",
    "\"\"\"\n",
    "#%load_ext autoreload\n",
    "#%autoreload\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "\n",
    "home = os.path.expanduser(\"~\");\n",
    "dsdir_ner = \"../datasets/ner\";\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "get_wikifify_params(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wikify import *\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"David\", \"met\", \"Victoria\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "S=[\"Phoenix, Arizona\"] \n",
    "M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "ids, titles = wikify(S,M,C, ws, method='context2context')\n",
    "\n",
    "\n",
    "#print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "print \"Best IDS\", ids, \"\\n\"\n",
    "print \"Best titles\", titles, \"\\n\"\n",
    "#print \"get_tp\",get_tp(M, ids) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy WSD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_legacy_eval.py \n",
    "'''The Legacy WSD evaluation\n",
    "This is how we evaluated for the thesis. \n",
    "The main difference is \"enforce=True\" in the candidate generation, \n",
    "an option which always makes sure the correct entity is among the candidates\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "from config import *\n",
    "np.seterr(all='raise')\n",
    "\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "# max_t = 15\n",
    "# max_count = 20\n",
    "# verbose = True\n",
    "\n",
    "fresh_restart=False\n",
    "\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(dsdir_ner, 'kore.json'),\n",
    "          os.path.join(dsdir_ner, 'wiki-mentions.5000.json'),\n",
    "          os.path.join(dsdir_ner, 'aida.json'),  \n",
    "          os.path.join(dsdir_ner, 'msnbc.txt.json'),\n",
    "          os.path.join(dsdir_ner, 'aquaint.txt.json') \n",
    "          ]\n",
    "\n",
    "methods = ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile', 'learned']\n",
    "methods = ['popularity','mention2entity','context2context','context2profile', 'learned']\n",
    "methods = ['context2context']\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd_legacy')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                \"rvspagerank\", method, \"both\", max_t, 5)\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join(['rvspagerank', str(DIR_BOTH), method, str(max_t), '5', os.path.basename(dsname)]))\n",
    "        \n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                \n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                    \n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=True)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wsd(S,M,C, method=method)\n",
    "                    tp = get_tp(ids, M) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        \n",
    "        detailedres ={\"dsname\":dsname, \"method\": \"rvspagerank\", \"op_method\": method, \"driection\": 'both',\n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": 5}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', 'rvspagerank', method, 'both', max_t , 5, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "        \n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_new_eval.py \n",
    "''' The new WSD evaluation method, here some of the options we already settled on\n",
    "    is fixed and hidden, such as similarity method or ws\n",
    "'''\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "from config import *\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "\n",
    "\n",
    "# max_t = 15\n",
    "# max_count = 20\n",
    "# verbose = True\n",
    "\n",
    "fresh_restart=False\n",
    "\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(dsdir_ner, 'kore.json'),\n",
    "          os.path.join(dsdir_ner, 'wiki-mentions.5000.json'),\n",
    "#          os.path.join(dsdir_ner, 'aida.json'),  \n",
    "          os.path.join(dsdir_ner, 'msnbc.txt.json'),\n",
    "          os.path.join(dsdir_ner, 'aquaint.txt.json') \n",
    "          ]\n",
    "\n",
    "methods = ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile', 'learned']\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd_new')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "ltr_nrows=10000\n",
    "load_wsd_model(ltr_nrows)\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, max_t: %s ...\"  % (dsname,\n",
    "                method, max_t)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(max_t), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wsd(S,M,C, method=method)\n",
    "                    tp = get_tp(ids, M) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"problem\": \"wsd\"}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, 'wikify_link\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, max_t, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikify_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikify_eval.py\n",
    "\"\"\"Evaluating the wsd module. It assumes the sentences are already segmented\n",
    "\"\"\"\n",
    "from wikify import *\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "import time\n",
    "from random import shuffle\n",
    "from config import *\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "\n",
    "\n",
    "# max_t = 20\n",
    "# max_count = 30\n",
    "# verbose = True\n",
    "\n",
    "fresh_restart=False\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(dsdir_ner, 'kore.json'),\n",
    "          os.path.join(dsdir_ner, 'wiki-mentions.5000.json'),\n",
    "#          os.path.join(dsdir_ner, 'aida.json'),  \n",
    "          os.path.join(dsdir_ner, 'msnbc.txt.json'),\n",
    "          os.path.join(dsdir_ner, 'aquaint.txt.json') \n",
    "          ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "allparams = (   (CORE_NLP,None, None, LTR_NROWS_L),\n",
    "#               (LEARNED_MENTION, SVC_HP_NROWS_S, SVC_HP_CV_S, LTR_NROWS_L), \n",
    "#               (LEARNED_MENTION, SVC_HR_NROWS_S, SVC_HR_CV_S, LTR_NROWS_L),\n",
    "              (LEARNED_MENTION, SVC_HP_NROWS_L, SVC_HP_CV_L, LTR_NROWS_L), \n",
    "              (LEARNED_MENTION, SVC_HR_NROWS_L, SVC_HR_CV_L, LTR_NROWS_L))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for mentionmethod, svc_nrows, svc_cv,  ltr_nrows in allparams:\n",
    "    load_wsd_model(ltr_nrows)\n",
    "    load_mention_model(svc_nrows, svc_cv)\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "\n",
    "        print \"dsname: %s, mentionmethods: %s, max_t: %s ...\"  % (dsname,\n",
    "                mentionmethod, max_t)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([str(mentionmethod), str(max_t), str(svc_nrows), str(svc_cv), str(ltr_nrows) ,os.path.basename(dsname)]))\n",
    "        mention_overall=[]\n",
    "        wikify_overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "\n",
    "                    if js['mention_measures'] is not None:\n",
    "                        mention_overall.append(js['mention_measures'])\n",
    "\n",
    "                    if js['wikify_measures'] is not None:\n",
    "                        wikify_overall.append(js['wikify_measures'])\n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "\n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                text= \" \".join(S)    \n",
    "                #S2,M2 = detect_mentions(text, mentionmethod=mentionmethod)      \n",
    "                S2,M2 = wikify_string(text, mentionmethod=mentionmethod)\n",
    "\n",
    "                mention_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=False)\n",
    "                mention_overall.append(mention_measures)\n",
    "\n",
    "                wikify_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=True)\n",
    "                wikify_overall.append(wikify_measures)\n",
    "\n",
    "                tmpf.write(json.dumps({\"no\":count, \"mention_measures\":mention_measures, \"wikify_measures\":wikify_measures})+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"mentionmethod\": mentionmethod, \"max_t\":max_t, \"svc_nrows\": svc_nrows, \"svc_cv\": svc_cv, \"ltr_nrows\":ltr_nrows, \"dsname\": dsname,\n",
    "                      \"max_t\": max_t, \"mention_overall\":mention_overall, \"wikify_overall\": wikify_overall, \"elapsed\": elapsed}\n",
    "\n",
    "\n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        #print mention_overall\n",
    "\n",
    "        mention_overall_measures = get_overall_measures(mention_overall)    \n",
    "        output = ('mention_evaluation',mentionmethod, max_t, svc_nrows, svc_cv, ltr_nrows, dsname) + mention_overall_measures + (elapsed,)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n', *output)\n",
    "\n",
    "        #print wikify_overall\n",
    "        wikify_overall_measures = get_overall_measures(wikify_overall)  \n",
    "        output = ('wikify_evaluation',mentionmethod, max_t,  svc_nrows, svc_cv, ltr_nrows, dsname) + wikify_overall_measures + (elapsed,)\n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n', *output)\n",
    "            \n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wikify_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikify_test.py\n",
    "from wikify import *\n",
    "\n",
    "svc_nrows, svc_cv = SVC_HP_NROWS_S, SVC_HP_CV_S\n",
    "\n",
    "load_mention_model(svc_nrows, svc_cv)\n",
    "\n",
    "ltr_nrows=10000\n",
    "load_wsd_model(ltr_nrows)\n",
    "\n",
    "S=[\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]\n",
    "\n",
    "S=[\"Nixon\", \"resigned\", \"after\", \"Watergate\", \"despite\", \"his\", \"success\", \"in\", \"the\", \"Ping-Pong Diplomacy\", \"with\", \"China\", \".\"]\n",
    "M=[[0, \"Richard_Nixon\"], [3, \"Watergate_scandal\"], [9, \"Ping_Pong_Diplomacy\"], [11, \"People's_Republic_of_China\"]]\n",
    "text = \" \".join(S).decode('utf-8')\n",
    "# print text\n",
    "#S, M, _ = annotate_with_corenlp(text) \n",
    "S2,M2 = detect_mentions(text, mentionmethod=LEARNED_MENTION)      \n",
    "#print M2\n",
    "S2,M2 = wikify_string(text, mentionmethod=LEARNED_MENTION)\n",
    "import cProfile as profile\n",
    "profile.run('wikify_string(text, mentionmethod=LEARNED_MENTION)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
