{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification\n",
    "This notebook contains code for doing wikification, as well as evaluating it.\n",
    "\n",
    "## Overview of our Wikification Method\n",
    "The following flowchart describes how our wikification method works without much technical detail.\n",
    "<img src=\"https://docs.google.com/drawings/d/19fInwE2C_fsAFiMNnIPe0cFldIHbJTHrXrLlLg6nbwI/pub?w=800&h=550\">\n",
    "<center><strong>Figure 1.</strong> A flowchart describing our wikification method at a relatively basic level.</center>\n",
    "\n",
    "## With More Detail\n",
    "In reference to figure 1.:\n",
    "\n",
    "### 1.  Input Some Text Here: [http://cgm6.research.cs.dal.ca/~amaral/wikisim/#wikifier](http://cgm6.research.cs.dal.ca/~amaral/wikisim/#wikifier)\n",
    "Self explanatory, just feed into the wikifier some text that is desired to be wikified. In the evaluation part of our code, the text comes pre-split from the datasets. We either keep the text split to focus more on our wikification, or join the text with spaces to evaluate while taking our own mention extraction into account.\n",
    "\n",
    "### 2. Tokenize Text Into Mentions\n",
    "The text can tokenized by a [Solr](http://lucene.apache.org/solr/) extension called [Solr Text Tagger](https://github.com/OpenSextant/SolrTextTagger) (STT), this tokenizer returns all potential mentions that it detects in the text. We can have STT return return all overlaps. So if given the text: 'The United States of America', the tokenizer would return all of 'The United States', 'The United States of America', 'United States', and 'United States of America'. Having these overlaps in the end are undesirable for our wikification purposes, as there should be no overlaps in the end. However It can be beneficial to enable the overlaps so that we can obtain more potential mentions that we can deal with later more intelligently than STT Can. Alternatively STT can deal with overlaps for us by naively choosing the longest tag in each overlap group, a more intelligent solution is obviously more desireable.\n",
    "\n",
    "The text may alternatively be tokenized by [CoreNLP entity mention annotator](https://stanfordnlp.github.io/CoreNLP/entitymentions.html). It works well for named entity recognition, because that is what it is designed for, however we do the more general entity regognition, which is supposed to capture more than just named entities. \n",
    "\n",
    "### 3. Filter Mentions with Classifier (If not doing CoreNLP)\n",
    "We use a veriety of features to help classify which of these mentions are actually mentions. The classifier was trained on data from Wikipedia. The features used on each mention for classification are: the POS tag (using [Natural Language Toolkit (NLTK)](http://www.nltk.org/)) of the mention, the word before it, and the word after it, the probability that the mention text linked to something in wikipedia (mention text appearences divided by mention link appearances), whether the CoreNLP entity mention annotator thought the mention is a mention, whether it starts with a capital, whether there is a page in wikipedia that has a matching title, whether it contains a space, and whether it contains only ascii character. Once all mentions are classified, if there are any overlaps they are removed in some way.\n",
    "\n",
    "### 4. Candidate Generation\n",
    "Now that the mentions are all extracted, we must generate a list of possible entities that each mention can refer to, we call these, entity candidates. To select n candidates, we first try selecting n/2 entities (Wikipedia page) that the given mention refers to most on Wikipedia. We refer to this measure as popularity. Once the n/2 most popular are selected, the remaining n/2 entities are selected based on the contextual similarity with other mentions in the same sentence, but mainly on how similar the title of an entity is to the mention string. If selecting most popular fails to return n/2 results, we try selecting however many we need based on context. So if popularity returns 0 entities, we select n from the most contextual. This method of mixing popular and contextual candidates is called the hybrid method.\n",
    "\n",
    "At the moment it is better to generate candidates by selecting the most popular only because that gives a better score in the entity linking. This is most likely due to a similar context score being used in both the candidate generation and entity linking.\n",
    "\n",
    "The hybrid method of candidate generation scores best on average on each of our datasets for correct entity recall. Selecting candidates based on popularity alone initially seemed more promising, because it scored better on the largest dataset (wiki5000), thus giving an overall higher recall. But on each independent dataset, the popularity method only scored better on wiki5000, by about one percent, whereas other datasets scored worse, from 3 to 13 percent. \n",
    "\n",
    "### 5. Candidate Scoring\n",
    "For each mention, all of the candidates must be scored on some metric. The candidate with the best score will be selected as the proposed entity for the mention. All of these methods rely on basic scores such as the popularity of an entity given the mention, or some measure of similarity from the context of a mention to the document of a candidate. Individually some of these methods perform well on select datasets, but combined together using machine learning gives the best results overall. Using a learning to rank algorithm ([LambdaMART](https://github.com/jma127/pyltr)), we achieve a score better than any best idividual on each of our datasets.\n",
    "\n",
    "#### Popularity\n",
    "This method simply chooses the most popular candidate (most popular as described in section 5 Candidate Generation). This method performs very well but is undesirable due to the fact that it is just blindly guessing, and could be horribly wrong in some cases. See [this comic](https://comic.hmp.is.it/comic/rainy-days/) for an example of someone who does not quite get this concept.\n",
    "\n",
    "#### Context 1\n",
    "For this method, the sentence that contains the mention is extracted and called the context. The mention is removed from this context. We then use Solr to search for the most similar document (Wikipedia page) by searching in the document text field for the context, as well as searching in the document title field for the mention text. The set of documents that we are searching through in Solr is of-course limited to those that are the candidates of the mention. We do what is called boosting to make the results more weighted by the title field, the results from this are boosted by 1.35 (multiplied) on each document. The document with the highest score is deemed the most similar and is selected as the proposed entity for the mention.\n",
    "\n",
    "#### Context 2\n",
    "This method is slightly similar to Context 1 as it also uses Solr and it uses the sentence as a context in the same way. The difference is that we use a different index for this method. The index for this method, rather than containing whole documents (Wikipedia pages), contains all instances of all mentions with the surrounding context of each mention as a record. For example, a record could be for the mention 'David', the record will also have n (5 in this example) words before the mention: 'is a soccer player named', n words after the mention: 'he played for Manchester United', the Wikipedia page that the mention is in, and the Wikipedia page that the mention refers to. Using this index we search in the collection of all records that have the mention refer our candidate, for each candidate. The n words before and after are searched in for our context sentence, whichever entity has the highest number of relevant examples is selected as the proposed entity for that mention.\n",
    "\n",
    "#### Word2Vec\n",
    "For this method we have Word2Vec create a vector space model of concepts from a Wikipedia corpus. The entities, as well as regular words all have their own vector representation. To use this method, we select n words before and after the mention, and get the vector representation of each of these words. All of these vectors are added together to become a context vector. This context vector is compared to the vector representation of each of the candidates. The candidate vector that is most similar (by cosine similarity) to the context vector is selected as the proposed entity for the mention. This method does not perform very well, and since it is basically just another context method we choose to leave it out while still attaining similar results.\n",
    "\n",
    "#### Coherence\n",
    "This method uses the reverse page rank algorithm to determine which combination of candidates from all mentions makes the most sense together. This method looks at the quality of all of the proposed entities from all mentions together, instead of individually selecting the proposed entity for each individual mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation Code\n",
    "The code in this cell is used to evaluate the precision and recall of the wikification code as well as other wikification methods.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### KORE\n",
    "* 50 records.\n",
    "* Relatively small pieces of text with the main goal of being tricky for wikification systems.\n",
    "\n",
    "### Aquaint\n",
    "* 50 records.\n",
    "* News.\n",
    "\n",
    "### MSNBC\n",
    "* 20 records.\n",
    "* News.\n",
    "\n",
    "### Wiki[n]\n",
    "* n records (we usually use 500 or 5000).\n",
    "* Opening paragraph of a variety of randomly selected Wikipedia articles.\n",
    "\n",
    "### nopop\n",
    "* 2304 records.\n",
    "* Comprised of subsets of the other datasets.\n",
    "* Only contains records where the most popular candidate is not the correct entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikification-eval.py\n",
    "\"\"\"\n",
    "This is for testing performance of wikification.\n",
    "\n",
    "Instruction: Simply change whatever parameters you want in this file, \n",
    "some parameters are only changeable in wikification.py, so that will \n",
    "have to be edited too. You can edit the 'comment' variable in the\n",
    "bottom of this script to describe what you are doing in the current\n",
    "test. Run this script with the command: 'python wikification-eval.py' \n",
    "without the quotes. \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from wikification import *\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import tagme\n",
    "import os\n",
    "import json\n",
    "from sets import Set\n",
    "\n",
    "tagme.GCUBE_TOKEN = \"f6c2ba6c-751b-4977-a94c-c140c30e9b92-843339462\"\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# many different option for combinations of datasets for smaller tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'wiki500', 'path':os.path.join(pathStrt,'wiki-mentions.500.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},{'name':'nopop', 'path':os.path.join(pathStrt,'nopop.json')}]\n",
    "\n",
    "# 'popular', 'context1', 'context2', 'word2vec', 'coherence', 'tagme', 'multi'\n",
    "methods = ['multi']\n",
    "# 'lmart', 'gbr', 'etr', 'rfr'\n",
    "mlModel = 'lmart' # to be used with method multi\n",
    "erMethod = 'cls1' # method for entity recognition / mention extraction\n",
    "\n",
    "if 'word2vec' in methods:\n",
    "    try:\n",
    "        word2vec\n",
    "    except:\n",
    "        word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "        \n",
    "# can do both, none would be pointless\n",
    "doSplit = True # mentions are given\n",
    "doManual = False # mentions not given\n",
    "\n",
    "verbose = True # decides how much stuff to ouput\n",
    "\n",
    "maxCands = 20 # amount of candidates for entity candidate generation (20 prefered)\n",
    "doHybrid = False # whether to do hybrid candidate generation (False prefered)\n",
    "\n",
    "\n",
    "performances = {} # record data here\n",
    "\n",
    "skipped = 0\n",
    "badThing = []\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    \n",
    "    # get all lines\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        \n",
    "    print '\\n' + dataset['name'] + '\\n'\n",
    "    \n",
    "    # run each method on the data set\n",
    "    for mthd in methods:\n",
    "        print mthd\n",
    "        print str(datetime.now()) + '\\n'\n",
    "        \n",
    "        ## reset counters\n",
    "        # micro scores\n",
    "        totalMicroPrecS = 0\n",
    "        totalMicroPrecM = 0\n",
    "        totalMicroRecS = 0\n",
    "        totalMicroRecM = 0\n",
    "        # macro scores\n",
    "        totalMacroPrecS = 0\n",
    "        totalMacroPrecM = 0\n",
    "        totalMacroRecS = 0\n",
    "        totalMacroRecM = 0\n",
    "        # BOT micro scores\n",
    "        totalBotMicroPrecS = 0\n",
    "        totalBotMicroPrecM = 0\n",
    "        totalBotMicroRecS = 0\n",
    "        totalBotMicroRecM = 0\n",
    "        # BOT macro scores\n",
    "        totalBotMacroPrecS = 0\n",
    "        totalBotMacroPrecM = 0\n",
    "        totalBotMacroRecS = 0\n",
    "        totalBotMacroRecM = 0\n",
    "        # amount of lines done in dataset\n",
    "        totalLines = 0\n",
    "        # amount of mentions in dataset\n",
    "        totalMentions = 0 \n",
    "        # amount of mentions found\n",
    "        totalMyMentionsS = 0\n",
    "        totalMyMentionsM = 0\n",
    "        \n",
    "        # each method tests all lines\n",
    "        for line in dataLines:\n",
    "            if verbose:\n",
    "                print str(totalLines + 1)\n",
    "            \n",
    "            # get absolute text indexes and entity id of each given mention\n",
    "            trueEntities = mentionStartsAndEnds(copy.deepcopy(line), forTruth = True) # the ground truth\n",
    "            \n",
    "            oData = copy.deepcopy(line) # copy of the line data\n",
    "            \n",
    "            # get results for pre split string\n",
    "            if doSplit and mthd <> 'tagme': # presplit no work on tagme\n",
    "                # original split string with mentions given\n",
    "                try:\n",
    "                    resultS = wikifyEval(copy.deepcopy(line), True, hybridC = doHybrid, maxC = maxCands, \n",
    "                                         method = mthd, model = mlModel, erMethod = erMethod)\n",
    "                except:\n",
    "                    skipped += 1\n",
    "                    badThing.append(line)\n",
    "                    continue\n",
    "                precS = precision(trueEntities, resultS) # precision of pre-split\n",
    "                recS = recall(trueEntities, resultS) # recall of pre-split\n",
    "                \n",
    "                # micro scores\n",
    "                totalMicroPrecS += len(resultS) * precS\n",
    "                totalMicroRecS += len(trueEntities) * recS\n",
    "                # macro scores\n",
    "                totalMacroPrecS += precS\n",
    "                totalMacroRecS += recS\n",
    "                \n",
    "                # get bot scores\n",
    "                trueSet = Set()\n",
    "                for truEnt in trueEntities:\n",
    "                    trueSet.add(truEnt[2])\n",
    "                mySet = Set()\n",
    "                for res in resultS:\n",
    "                    mySet.add(res[2])\n",
    "                    \n",
    "                try:\n",
    "                    precS = len(trueSet & mySet)/len(mySet)\n",
    "                except:\n",
    "                    precS = 0\n",
    "                    \n",
    "                try:\n",
    "                    recS = len(trueSet & mySet)/len(trueSet)\n",
    "                except:\n",
    "                    recS = 0\n",
    "                \n",
    "                # BOT micro scores\n",
    "                totalBotMicroPrecS += len(trueSet) * precS\n",
    "                totalBotMicroRecS += len(trueSet) * recS\n",
    "                # BOT macro scores\n",
    "                totalBotMacroPrecS += precS\n",
    "                totalBotMacroRecS += recS\n",
    "                \n",
    "                totalMyMentionsS += len(resultS)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Split: ' + str(precS) + ', ' + str(recS)\n",
    "                \n",
    "            # get results for manually split string\n",
    "            if doManual:\n",
    "                # tagme has separate way to do things\n",
    "                if mthd == 'tagme':\n",
    "                    antns = tagme.annotate(\" \".join(line['text']))\n",
    "                    resultM = []\n",
    "                    for an in antns.get_annotations(0.005):\n",
    "                        resultM.append([an.begin,an.end,title2id(an.entity_title)])\n",
    "                else:\n",
    "                    # unsplit string to be manually split and mentions found\n",
    "                    try:\n",
    "                        resultM = wikifyEval(\" \".join(line['text']), False, hybridC = doHybrid, \n",
    "                                         maxC = maxCands, method = mthd, model = mlModel, erMethod = erMethod)\n",
    "                    except:\n",
    "                        skipped += 1\n",
    "                        badThing.append(line)\n",
    "                        continue\n",
    "                \n",
    "                precM = precision(trueEntities, resultM) # precision of manual split\n",
    "                recM = recall(trueEntities, resultM) # recall of manual split\n",
    "                \n",
    "                \"\"\"\n",
    "                I think the math for micro scores are wrong in manual\n",
    "                \"\"\"\n",
    "                \n",
    "                # micro scores\n",
    "                totalMicroPrecM += len(trueEntities) * precM\n",
    "                totalMicroRecM += len(trueEntities) * recM\n",
    "                # macro scores\n",
    "                totalMacroPrecM += precM\n",
    "                totalMacroRecM += recM\n",
    "                \n",
    "                # get bot scores\n",
    "                trueSet = Set()\n",
    "                for truEnt in trueEntities:\n",
    "                    trueSet.add(truEnt[2])\n",
    "                mySet = Set()\n",
    "                for res in resultM:\n",
    "                    mySet.add(res[2])\n",
    "                    \n",
    "                try:\n",
    "                    precM = len(trueSet & mySet)/len(mySet)\n",
    "                except:\n",
    "                    precM = 0\n",
    "                    \n",
    "                try:\n",
    "                    recM = len(trueSet & mySet)/len(trueSet)\n",
    "                except:\n",
    "                    recM = 0\n",
    "                \n",
    "                # BOT micro scores\n",
    "                totalBotMicroPrecM += len(trueSet) * precM\n",
    "                totalBotMicroRecM += len(trueSet) * recM\n",
    "                # BOT macro scores\n",
    "                totalBotMacroPrecM += precM\n",
    "                totalBotMacroRecM += recM\n",
    "                \n",
    "                totalMyMentionsM += len(resultM)\n",
    "                \n",
    "                if verbose:\n",
    "                    print 'Manual: ' + str(precM) + ', ' + str(recM)\n",
    "                \n",
    "            totalLines += 1\n",
    "            totalMentions += len(trueEntities)\n",
    "        \n",
    "        # record results for this method on this dataset\n",
    "        # all F1 scores are put in later to avoid division by 0 possibility\n",
    "        \n",
    "        # to stop errors\n",
    "        if totalMyMentionsS == 0:\n",
    "            totalMyMentionsS = -1\n",
    "        if totalMyMentionsM == 0:\n",
    "            totalMyMentionsM = -1\n",
    "        if totalMentions == 0:\n",
    "            totalMentions = -1\n",
    "        \n",
    "        performances[dataset['name']][mthd] = {\n",
    "                   'S Micro Prec':totalMicroPrecS/totalMyMentionsS, \n",
    "                   'M Micro Prec':totalMicroPrecM/totalMyMentionsM,\n",
    "                   'S Micro Rec':totalMicroRecS/totalMentions, \n",
    "                   'M Micro Rec':totalMicroRecM/totalMentions,\n",
    "\n",
    "                   'S Macro Prec':totalMacroPrecS/totalLines,\n",
    "                   'M Macro Prec':totalMacroPrecM/totalLines,\n",
    "                   'S Macro Rec':totalMacroRecS/totalLines, \n",
    "                   'M Macro Rec':totalMacroRecM/totalLines,\n",
    "\n",
    "                   'S BOT Micro Prec':totalBotMicroPrecS/totalMyMentionsS, \n",
    "                   'M BOT Micro Prec':totalBotMicroPrecM/totalMyMentionsM,\n",
    "                   'S BOT Micro Rec':totalBotMicroRecS/totalMentions, \n",
    "                   'M BOT Micro Rec':totalBotMicroRecM/totalMentions,\n",
    "\n",
    "                   'S BOT Macro Prec':totalBotMacroPrecS/totalLines,\n",
    "                   'M BOT Macro Prec':totalBotMacroPrecM/totalLines,\n",
    "                   'S BOT Macro Rec':totalBotMacroRecS/totalLines, \n",
    "                   'M BOT Macro Rec':totalBotMacroRecM/totalLines\n",
    "                   }\n",
    "        \n",
    "        perf = performances[dataset['name']][mthd]\n",
    "        # hande divisions by 0 for F1 scores here\n",
    "        # s and m micro f1\n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['S Micro F1'] = (\n",
    "                    (2*perf['S Micro Prec']*perf['S Micro Rec'])/(perf['S Micro Prec']+perf['S Micro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['S Micro F1'] = 0\n",
    "            \n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['M Micro F1'] = (\n",
    "                    (2*perf['M Micro Prec']*perf['M Micro Rec'])/(perf['M Micro Prec']+perf['M Micro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['M Micro F1'] = 0\n",
    "        \n",
    "        # s and m macro f1\n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['S Macro F1'] = (\n",
    "                    (2*perf['S Macro Prec']*perf['S Macro Rec'])/(perf['S Macro Prec']+perf['S Macro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['S Macro F1'] = 0\n",
    "            \n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['M Macro F1'] = (\n",
    "                    (2*perf['M Macro Prec']*perf['M Macro Rec'])/(perf['M Macro Prec']+perf['M Macro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['M Macro F1'] = 0\n",
    "        \n",
    "        # s and m, micro and macro for BOT f1\n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['S BOT Micro F1'] = (\n",
    "                    (2*perf['S BOT Micro Prec']*perf['S BOT Micro Rec'])/(perf['S BOT Micro Prec']+perf['S BOT Micro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['S BOT Micro F1'] = 0\n",
    "            \n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['M BOT Micro F1'] = (\n",
    "                    (2*perf['M BOT Micro Prec']*perf['M BOT Micro Rec'])/(perf['M BOT Micro Prec']+perf['M BOT Micro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['M BOT Micro F1'] = 0\n",
    "        \n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['S BOT Macro F1'] = (\n",
    "                    (2*perf['S BOT Macro Prec']*perf['S BOT Macro Rec'])/(perf['S BOT Macro Prec']+perf['S BOT Macro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['S BOT Macro F1'] = 0\n",
    "            \n",
    "        try:\n",
    "            performances[dataset['name']][mthd]['M BOT Macro F1'] = (\n",
    "                    (2*perf['M BOT Micro Prec']*perf['M BOT Micro Rec'])/(perf['M BOT Micro Prec']+perf['M BOT Micro Rec']))\n",
    "        except:\n",
    "            performances[dataset['name']][mthd]['M BOT Macro F1'] = 0\n",
    "        \n",
    "print 'Skipped', skipped\n",
    "print badThing\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/wikification_results.txt', 'a') as resultFile:\n",
    "    \n",
    "    resultFile.write('\\n' + str(datetime.now()) + '\\n' \n",
    "                     + 'maxCands: ' + str(maxCands) + '\\n'\n",
    "                     + 'mlModel: ' + mlModel + '\\n'\n",
    "                     + 'erMethod: ' + erMethod + '\\n'\n",
    "                     + 'doHybrid: ' + str(doHybrid) + '\\n'\n",
    "                     + str(datetime.now()) + '\\n\\n')\n",
    "    \n",
    "    comment = 'Please make sure you delete this test.'\n",
    "    resultFile.write('Comment: ' + comment + '\\n\\n')\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        for mthd in methods:\n",
    "            if doSplit and doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Micro Prec :' + str(performances[dataset['name']][mthd]['S Micro Prec'])\n",
    "                       + '\\n    S Micro Rec :' + str(performances[dataset['name']][mthd]['S Micro Rec']) \n",
    "                       + '\\n    S Micro F1 :' + str(performances[dataset['name']][mthd]['S Micro F1'])\n",
    "                       + '\\n    S Macro Prec :' + str(performances[dataset['name']][mthd]['S Macro Prec'])\n",
    "                       + '\\n    S Macro Rec :' + str(performances[dataset['name']][mthd]['S Macro Rec']) \n",
    "                       + '\\n    S Macro F1 :' + str(performances[dataset['name']][mthd]['S Macro F1'])\n",
    "                       + '\\n    S BOT Micro Prec :' + str(performances[dataset['name']][mthd]['S BOT Micro Prec'])\n",
    "                       + '\\n    S BOT Micro Rec :' + str(performances[dataset['name']][mthd]['S BOT Micro Rec']) \n",
    "                       + '\\n    S BOT Micro F1 :' + str(performances[dataset['name']][mthd]['S BOT Micro F1'])\n",
    "                       + '\\n    S BOT Macro Prec :' + str(performances[dataset['name']][mthd]['S BOT Macro Prec'])\n",
    "                       + '\\n    S BOT Macro Rec :' + str(performances[dataset['name']][mthd]['S BOT Macro Rec']) \n",
    "                       + '\\n    S BOT Macro F1 :' + str(performances[dataset['name']][mthd]['S BOT Macro F1']) + '\\n'\n",
    "                       + '\\n    M Micro Prec :' + str(performances[dataset['name']][mthd]['M Micro Prec'])\n",
    "                       + '\\n    M Micro Rec :' + str(performances[dataset['name']][mthd]['M Micro Rec']) \n",
    "                       + '\\n    M Micro F1 :' + str(performances[dataset['name']][mthd]['M Micro F1'])\n",
    "                       + '\\n    M Macro Prec :' + str(performances[dataset['name']][mthd]['M Macro Prec'])\n",
    "                       + '\\n    M Macro Rec :' + str(performances[dataset['name']][mthd]['M Macro Rec']) \n",
    "                       + '\\n    M Macro F1 :' + str(performances[dataset['name']][mthd]['M Macro F1'])\n",
    "                       + '\\n    M BOT Micro Prec :' + str(performances[dataset['name']][mthd]['M BOT Micro Prec'])\n",
    "                       + '\\n    M BOT Micro Rec :' + str(performances[dataset['name']][mthd]['M BOT Micro Rec']) \n",
    "                       + '\\n    M BOT Micro F1 :' + str(performances[dataset['name']][mthd]['M BOT Micro F1'])\n",
    "                       + '\\n    M BOT Macro Prec :' + str(performances[dataset['name']][mthd]['M BOT Macro Prec'])\n",
    "                       + '\\n    M BOT Macro Rec :' + str(performances[dataset['name']][mthd]['M BOT Macro Rec']) \n",
    "                       + '\\n    M BOT Macro F1 :' + str(performances[dataset['name']][mthd]['M BOT Macro F1']) + '\\n')\n",
    "            elif doSplit:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    S Micro Prec :' + str(performances[dataset['name']][mthd]['S Micro Prec'])\n",
    "                       + '\\n    S Micro Rec :' + str(performances[dataset['name']][mthd]['S Micro Rec']) \n",
    "                       + '\\n    S Micro F1 :' + str(performances[dataset['name']][mthd]['S Micro F1'])\n",
    "                       + '\\n    S Macro Prec :' + str(performances[dataset['name']][mthd]['S Macro Prec'])\n",
    "                       + '\\n    S Macro Rec :' + str(performances[dataset['name']][mthd]['S Macro Rec']) \n",
    "                       + '\\n    S Macro F1 :' + str(performances[dataset['name']][mthd]['S Macro F1'])\n",
    "                       + '\\n    S BOT Micro Prec :' + str(performances[dataset['name']][mthd]['S BOT Micro Prec'])\n",
    "                       + '\\n    S BOT Micro Rec :' + str(performances[dataset['name']][mthd]['S BOT Micro Rec']) \n",
    "                       + '\\n    S BOT Micro F1 :' + str(performances[dataset['name']][mthd]['S BOT Micro F1'])\n",
    "                       + '\\n    S BOT Macro Prec :' + str(performances[dataset['name']][mthd]['S BOT Macro Prec'])\n",
    "                       + '\\n    S BOT Macro Rec :' + str(performances[dataset['name']][mthd]['S BOT Macro Rec']) \n",
    "                       + '\\n    S BOT Macro F1 :' + str(performances[dataset['name']][mthd]['S BOT Macro F1']) + '\\n')\n",
    "            elif doManual:\n",
    "                resultFile.write(mthd + ':'\n",
    "                       + '\\n    M Micro Prec :' + str(performances[dataset['name']][mthd]['M Micro Prec'])\n",
    "                       + '\\n    M Micro Rec :' + str(performances[dataset['name']][mthd]['M Micro Rec']) \n",
    "                       + '\\n    M Micro F1 :' + str(performances[dataset['name']][mthd]['M Micro F1'])\n",
    "                       + '\\n    M Macro Prec :' + str(performances[dataset['name']][mthd]['M Macro Prec'])\n",
    "                       + '\\n    M Macro Rec :' + str(performances[dataset['name']][mthd]['M Macro Rec']) \n",
    "                       + '\\n    M Macro F1 :' + str(performances[dataset['name']][mthd]['M Macro F1'])\n",
    "                       + '\\n    M BOT Micro Prec :' + str(performances[dataset['name']][mthd]['M BOT Micro Prec'])\n",
    "                       + '\\n    M BOT Micro Rec :' + str(performances[dataset['name']][mthd]['M BOT Micro Rec']) \n",
    "                       + '\\n    M BOT Micro F1 :' + str(performances[dataset['name']][mthd]['M BOT Micro F1'])\n",
    "                       + '\\n    M BOT Macro Prec :' + str(performances[dataset['name']][mthd]['M BOT Macro Prec'])\n",
    "                       + '\\n    M BOT Macro Rec :' + str(performances[dataset['name']][mthd]['M BOT Macro Rec']) \n",
    "                       + '\\n    M BOT Macro F1 :' + str(performances[dataset['name']][mthd]['M BOT Macro F1']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mention_extraction_eval.py \n",
    "\n",
    "\"\"\"\n",
    "This evaluates the quality of mention extraction, macro scores.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from wikification import *\n",
    "from datetime import datetime\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "#pathStrt = 'C:\\\\Temp\\\\wsd-datasets'\n",
    "\n",
    "# the data sets for performing on\n",
    "datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')},\n",
    "            {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')},\n",
    "            {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')},\n",
    "            {'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "\n",
    "# short for quick tests\n",
    "#datasets = [{'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}]\n",
    "#datasets = [{'name':'wiki5000', 'path':os.path.join(pathStrt,'wiki-mentions.5000.json')}]\n",
    "#datasets = [{'name':'kore', 'path':os.path.join(pathStrt,'kore.json')}, {'name':'AQUAINT', 'path':os.path.join(pathStrt,'AQUAINT.txt.json')}, {'name':'MSNBC', 'path':os.path.join(pathStrt,'MSNBC.txt.json')}]\n",
    "\n",
    "performances = {}\n",
    "\n",
    "verbose = True\n",
    "\n",
    "# for each dataset, run all methods\n",
    "for dataset in datasets:\n",
    "    performances[dataset['name']] = {}\n",
    "    # get the data from dataset\n",
    "    dataFile = open(dataset['path'], 'r')\n",
    "    dataLines = []\n",
    "    for line in dataFile:\n",
    "        dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "    \n",
    "    # reset counters\n",
    "    totalPrec = 0\n",
    "    totalRec = 0\n",
    "    totalF1 = 0\n",
    "    totalLines = 0\n",
    "\n",
    "    # each method tests all lines\n",
    "    for line in dataLines:\n",
    "\n",
    "        if(verbose):\n",
    "            print str(totalLines + 1)\n",
    "\n",
    "        trueMentions = mentionStartsAndEnds(line, True)\n",
    "        \n",
    "        \"\"\"\n",
    "        output = nlp.annotate(\" \".join(line['text']).encode('utf-8'), properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        myMentions = []\n",
    "        for sentence in output['sentences']:\n",
    "            for em in sentence['entitymentions']:\n",
    "                myMentions.append([em['characterOffsetBegin'], em['characterOffsetEnd']])\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\"\"\"\n",
    "        myMentions = mentionExtract(\" \".join(line['text']), mthd = 'cls1')['mentions']\n",
    "        \n",
    "        # put in right format\n",
    "        #print trueMentions\n",
    "        #print myMentions\n",
    "        for mention in myMentions:\n",
    "            mention[0] = mention[1]\n",
    "            mention[1] = mention[2]\n",
    "        \"\"\"\"\"\"\n",
    "            \n",
    "        prec = mentionPrecision(trueMentions, myMentions)\n",
    "        rec = mentionRecall(trueMentions, myMentions)\n",
    "        try:\n",
    "            f1 = (2*prec*rec)/(prec+rec)\n",
    "        except:\n",
    "            f1 = 0\n",
    "        \n",
    "        if(verbose):\n",
    "            print str(prec) + ' ' + str(rec) + ' ' + str(f1) + '\\n'\n",
    "\n",
    "        # track results\n",
    "        totalPrec += prec\n",
    "        totalRec += rec\n",
    "        totalF1 += f1\n",
    "        totalLines += 1\n",
    "\n",
    "    # record results for this method on this dataset\n",
    "    performances[dataset['name']] = {'Precision':totalPrec/totalLines, \n",
    "                                     'Recall':totalRec/totalLines,\n",
    "                                     'F1':totalF1/totalLines}\n",
    "            \n",
    "with open('/users/cs/amaral/wikisim/wikification/mention_extraction_results.txt', 'a') as resultFile:\n",
    "    resultFile.write(str(datetime.now()) + '\\n')\n",
    "    resultFile.write('Using cls1 (longest dominant right) with initial filter and limit amount.' + '\\n\\n')\n",
    "    for dataset in datasets:\n",
    "        resultFile.write(dataset['name'] + ':\\n')\n",
    "        resultFile.write('\\n    Prec :' + str(performances[dataset['name']]['Precision'])\n",
    "               + '\\n    Rec :' + str(performances[dataset['name']]['Recall'])\n",
    "               + '\\n    F1 :' + str(performances[dataset['name']]['F1']) + '\\n')\n",
    "                \n",
    "    resultFile.write('\\n' + '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Wikification Code\n",
    "## The code in this cell contains all of the logic to do wikification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikification.py\n",
    "from __future__ import division\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "sys.path.append('./pyltr/')\n",
    "import pyltr\n",
    "sys.path.append('../wikisim/')\n",
    "from wikipedia import *\n",
    "from operator import itemgetter\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg\n",
    "from calcsim import *\n",
    "import numpy as np\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "scnlp = StanfordCoreNLP('http://localhost:9000')\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "MIN_MENTION_LENGTH = 3 # mentions must be at least this long\n",
    "MIN_FREQUENCY = 20 # anchor with frequency below is ignored\n",
    "\n",
    "# with open('/users/cs/amaral/wikisim/wikification/pos-filter-out-nonmentions.txt', 'r') as srcFile:\n",
    "#     posFilter = srcFile.read().splitlines()\n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "\n",
    "    q='+text:(\\\"%s\\\")'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    try:\n",
    "        if 'response' not in r.json():\n",
    "            return 0\n",
    "        else:\n",
    "            return r.json()['response']['numFound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    result = anchor2concept(s)\n",
    "    rSum = 0\n",
    "    for item in result:\n",
    "        rSum += item[1]\n",
    "        \n",
    "    return rSum\n",
    "wi\n",
    "def mentionProb(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalMentions = get_mention_count(text)\n",
    "    totalAppearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    \n",
    "    if totalAppearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    else:\n",
    "        return totalMentions/totalAppearances\n",
    "    \n",
    "def normalize(nums):\n",
    "    \"\"\"Normalizes a list of nums to its sum + 1\"\"\"\n",
    "    \n",
    "    numSum = sum(nums) + 1 # get max\n",
    "    \n",
    "    # fill with normalized\n",
    "    normNums = []\n",
    "    for num in nums:\n",
    "        normNums.append(num/numSum)\n",
    "        \n",
    "    return normNums\n",
    "\n",
    "def destroyExclusiveOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps that start at same letter from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # textData minus the unesescary parts of the overlapping\n",
    "    overlappingSets = [] # stores arrays of the indexes of overlapping items from textData\n",
    "    \n",
    "    # creates the overlappingSets array\n",
    "    i = 0\n",
    "    while i < len(textData)-1:\n",
    "        # even single elements considered overlapping set\n",
    "        # this is root of overlapping set\n",
    "        overlappingSets.append([i])\n",
    "        overlapIndex = len(overlappingSets) - 1\n",
    "        theBegin = textData[i][0]\n",
    "        \n",
    "        # look at next words until not overlap\n",
    "        for j in range(i+1, len(textData)):\n",
    "            # if next word starts before endiest one ends\n",
    "            if textData[j][0] == theBegin:\n",
    "                overlappingSets[overlapIndex].append(j)\n",
    "                i = j # make sure not to repeat overlap set\n",
    "            else:\n",
    "                # add final word\n",
    "                if j == len(textData) - 1:\n",
    "                    overlappingSets.append([j])\n",
    "                break\n",
    "        i += 1\n",
    "                    \n",
    "    # get only the best overlapping element of each set\n",
    "    for oSet in overlappingSets:\n",
    "        bestIndex = 0\n",
    "        bestScore = -1\n",
    "        for i in oSet:\n",
    "            score = mentionProb(textData[i][2])\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestIndex = i\n",
    "        \n",
    "        # put right item in new textData\n",
    "        newTextData.append(textData[bestIndex])\n",
    "        \n",
    "    return newTextData\n",
    "\n",
    "def destroyResidualOverlaps(textData):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Removes all overlaps from text data, so that only the best mention in an\n",
    "        overlap set is left.\n",
    "    Args:\n",
    "        textData: [[start, end, text, anchProb],...]\n",
    "    Return:\n",
    "        textData minus the unesescary elements that overlap.\n",
    "    \"\"\"\n",
    "    \n",
    "    newTextData = [] # to be returned\n",
    "    oSet = [] # the set of current overlaps\n",
    "    rootWIndex = 0 # the word to start looking from for finding root word\n",
    "    rEnd = 0 # the end index of the root word\n",
    "    \n",
    "    # keep looping as long as overlaps\n",
    "    while True:\n",
    "        oSet = []\n",
    "        oSet.append(textData[rootWIndex])\n",
    "        for i in range(rootWIndex + 1, len(textData)):\n",
    "            # if cur start before root end\n",
    "            if textData[i][0] < textData[rootWIndex][1]:\n",
    "                oSet.append(textData[i])\n",
    "            else:\n",
    "                break # have all overlap words\n",
    "\n",
    "        \n",
    "        bestIndex = 0\n",
    "        # deal with the overlaps\n",
    "        if len(oSet) > 1:\n",
    "            bestProb = 0\n",
    "            \n",
    "            # choose the most probable\n",
    "            i = 0\n",
    "            for mention in oSet:\n",
    "                prob = mentionProb(mention[2])\n",
    "                if prob > bestProb:\n",
    "                    bestProb = prob\n",
    "                    bestIndex = i\n",
    "                i += 1\n",
    "        else:\n",
    "            rootWIndex += 1 # move up one if no overlaps\n",
    "                \n",
    "        # remove from old text data all that is not best\n",
    "        for i in range(0, len(oSet)):\n",
    "            if i <> bestIndex:\n",
    "                textData.remove(oSet[i])\n",
    "                \n",
    "        # add the best to new\n",
    "        if not (oSet[bestIndex] in newTextData):\n",
    "            newTextData.append(oSet[bestIndex])\n",
    "            \n",
    "        if rootWIndex >= len(textData):\n",
    "            break\n",
    "    \n",
    "    return newTextData\n",
    "    \n",
    "def mentionStartsAndEnds(textData, forTruth = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a list of mentions and turns each of its mentions into the form: [wIndex, start, end]. \n",
    "        Or if forTruth is true: [[start,end,entityId]]\n",
    "    Args:\n",
    "        textData: {'text': [w1,w2,w3,...] , 'mentions': [[wordIndex,entityTitle],...]}, to be transformed \n",
    "            as described above.\n",
    "        forTruth: Changes form to use.\n",
    "    Return:\n",
    "        The mentions in the form [[wIndex, start, end],...]]. Or if forTruth is true: [[start,end,entityId]]\n",
    "    \"\"\"\n",
    "    \n",
    "    curWord = 0 \n",
    "    curStart = 0\n",
    "    for mention in textData['mentions']:\n",
    "        while curWord < mention[0]:\n",
    "            curStart += len(textData['text'][curWord]) + 1\n",
    "            curWord += 1\n",
    "            \n",
    "        ent = mention[1] # store entity title in case of forTruth\n",
    "        mention.pop() # get rid of entity text\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.pop() # get rid of wIndex too\n",
    "            \n",
    "        mention.append(curStart) # start of the mention\n",
    "        mention.append(curStart + len(textData['text'][curWord])) # end of the mention\n",
    "        \n",
    "        if forTruth:\n",
    "            mention.append(title2id(ent)) # put on entityId\n",
    "    \n",
    "    return textData['mentions']\n",
    "\n",
    "posBefDict = {\n",
    "    'IN':0,\n",
    "    'DT':1,\n",
    "    'NNP':2,\n",
    "    'JJ':3,\n",
    "    ',':4,\n",
    "    'CC':5,\n",
    "    'NN':6,\n",
    "    'VBD':7,\n",
    "    'CD':8,\n",
    "    '(':9,\n",
    "    'TO':10,\n",
    "    'FAIL':11\n",
    "}\n",
    "\n",
    "posCurDict = {\n",
    "    'NNP':0,\n",
    "    'NN':1,\n",
    "    'JJ':2,\n",
    "    'NNS':3,\n",
    "    'CD':4,\n",
    "    'NNPS':5,\n",
    "    'FAIL':6\n",
    "}\n",
    "\n",
    "posAftDict = {\n",
    "    ',':0,\n",
    "    '.':1,\n",
    "    'IN':2,\n",
    "    'NNP':3,\n",
    "    'CC':4,\n",
    "    'NN':5,\n",
    "    'VBD':6,\n",
    "    ':':7,\n",
    "    'VBZ':8,\n",
    "    'POS':9,\n",
    "    'NNS':10,\n",
    "    'TO':11,\n",
    "    'FAIL':12\n",
    "}\n",
    "     \n",
    "def getGoodMentions(splitText, mentions, model, overlapFix = False):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "        Finds the potential mentions that are deemed good by our classifier.\n",
    "    Args:\n",
    "        splitText: The text in split form.\n",
    "        mentions: All of the potential mentions [word index, start offset, end offset].\n",
    "        model: The machine learning model to predict with.\n",
    "        overlapFix: Whether there are overlaps that need to be dealt with.\n",
    "    Return:\n",
    "        A subset of arg mentions with each element deemed to be worthy by the classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    goodMentions = [] # the mentions to return\n",
    "    \n",
    "    #Get POS tags of all text\n",
    "    postrs = nltk.pos_tag(copy.deepcopy(splitText))\n",
    "\n",
    "    # get stanford core mentions\n",
    "    try:\n",
    "        try:\n",
    "            atext = \" \".join(splitText).encode('utf-8')\n",
    "        except:\n",
    "            atext = \" \".join(splitText)\n",
    "        stnfrdMentions0 = scnlp.annotate(atext, properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'})\n",
    "    except:\n",
    "        print 'Error!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n",
    "        k = 1/0\n",
    "    stnfrdMentions = []\n",
    "    for sentence in stnfrdMentions0['sentences']:\n",
    "        for mention in sentence['entitymentions']:\n",
    "            stnfrdMentions.append(mention['text'])\n",
    "            \n",
    "    enc = OneHotEncoder(n_values = [12,7,13], categorical_features = [0,1,2])\n",
    "            \n",
    "    for i in range(len(mentions)):\n",
    "        aMention = [] # fill with attributes about current mention for prediction\n",
    "        \n",
    "        \"\"\" \n",
    "        Append POS tags of before, on, and after mention.\n",
    "        \"\"\"\n",
    "        if i == 0:\n",
    "            bef = 'NONE'\n",
    "        else:\n",
    "            bef = postrs[i-1][1] # pos tag of before\n",
    "        if bef in posBefDict:\n",
    "            bef = posBefDict[bef]\n",
    "        else:\n",
    "            bef = posBefDict['FAIL']\n",
    "            \n",
    "        on = postrs[i][1] # pos tag of mention\n",
    "        if on in posCurDict:\n",
    "            on = posCurDict[on]\n",
    "        else:\n",
    "            on = posCurDict['FAIL']\n",
    "        \n",
    "        if i == len(splitText) - 1:\n",
    "            aft = 'NONE'\n",
    "        else:\n",
    "            aft = postrs[i+1][1] # pos tag of after\n",
    "        if aft in posAftDict:\n",
    "            aft = posAftDict[aft]\n",
    "        else:\n",
    "            aft = posAftDict['FAIL']\n",
    "        \n",
    "        aMention.extend([bef, on, aft])\n",
    "        \n",
    "        \"\"\"\n",
    "        Append mention probability.\n",
    "        \"\"\"\n",
    "        aMention.append(mentionProb(splitText[mentions[i][0]]))\n",
    "        \n",
    "        \"\"\"\n",
    "        Find whether Stanford NER decides the word to be mention.\n",
    "        \"\"\"\n",
    "        if splitText[mentions[i][0]] in stnfrdMentions:\n",
    "            stnfrdMentions.remove(splitText[mentions[i][0]])\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether starts with capital.\n",
    "        \"\"\"\n",
    "        if splitText[mentions[i][0]][0].isupper():\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether there is an exact match in Wikipedia.\n",
    "        \"\"\"\n",
    "        if title2id(splitText[mentions[i][0]]) is not None:\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether word contains a space.\n",
    "        \"\"\"\n",
    "        if ' ' in splitText[mentions[i][0]]:\n",
    "            aMention.append(1)\n",
    "        else:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Whether the word contains only ascii characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            splitText[mentions[i][0]].decode('ascii')\n",
    "            aMention.append(1)\n",
    "        except:\n",
    "            aMention.append(0)\n",
    "            \n",
    "        \"\"\"\n",
    "        Get all positive classified instances.\n",
    "        \"\"\"\n",
    "        #aMention = enc.fit_transform([aMention]).toarray()[0]\n",
    "        if model.predict([aMention])[0] == 1:\n",
    "            goodMentions.append(mentions[i])\n",
    "            # put score of prediction\n",
    "            goodMentions[-1].append(model.predict_proba([aMention])[0][1]) # put score of prediction\n",
    "            \n",
    "    \n",
    "    # get the right amount needed\n",
    "    if True:\n",
    "        amount = 5 + int(0.12778 * len(splitText))\n",
    "    else:\n",
    "        amount = len(goodMentions)\n",
    "            \n",
    "    if overlapFix == False:\n",
    "        goodMentions = sorted(goodMentions, key = itemgetter(1), reverse = False)[:amount]\n",
    "        return goodMentions        \n",
    "    else:\n",
    "        \"\"\"\n",
    "        Remove all overlaps in results.\n",
    "        \"\"\"\n",
    "            \n",
    "        # sort on prediction probability descending\n",
    "        goodMentions = sorted(goodMentions, key = itemgetter(-1), reverse = True)\n",
    "\n",
    "        try:\n",
    "            goodlen = len(goodMentions[0])\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "        for mention1 in goodMentions:\n",
    "            if len(mention1) > goodlen:\n",
    "                continue\n",
    "            for mention2 in goodMentions:\n",
    "                # dont do anything with a previous or same one\n",
    "                if (mention2[0] == mention1[0] or\n",
    "                        mention1[-1] < mention2[-1] or\n",
    "                        len(mention2) > goodlen):\n",
    "                    continue\n",
    "                # flag 2 if 2 starts before 1 ends and 2 ends after 1 starts\n",
    "                if mention2[1] < mention1[2] and mention2[2] >= mention1[1]:\n",
    "                    #print 'Overlap found', str(mention1), str(mention2)\n",
    "                    mention2.append(0) # just increase length to flag for deletion\n",
    "\n",
    "        finalMentions = []\n",
    "        for mention in goodMentions:\n",
    "            if len(mention) == goodlen:\n",
    "                finalMentions.append(mention[:3])\n",
    "                \n",
    "        finalMentions = sorted(finalMentions, key = itemgetter(1), reverse = False)[:amount]\n",
    "\n",
    "        return finalMentions\n",
    "    \n",
    "def mentionExtract(text, mthd = 'cls2'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in a text and splits it into the different words/mentions.\n",
    "    Args:\n",
    "        text: The text to be split.\n",
    "        useCoreNLP: Whether to use CoreNLP entity mention annotation.\n",
    "            Currently severely broken, never set to True.\n",
    "    Return:\n",
    "        The text split it into the different words / mentions: \n",
    "        {'text':[w1,w2,...], 'mentions': [[wIndex,begin,end],...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    if mthd == 'cnlp': # use CoreNLP's entity mention annotator\n",
    "        output = scnlp.annotate(text, properties={\n",
    "            'annotators': 'entitymentions',\n",
    "            'outputFormat': 'json'\n",
    "        })\n",
    "        \n",
    "        # get all tokens together and all mentions together\n",
    "        tokens = []\n",
    "        mentions0 = []\n",
    "        for sentence in output['sentences']:\n",
    "            for token in sentence['tokens']:\n",
    "                tokens.append(token['originalText'])\n",
    "            for em in sentence['entitymentions']:\n",
    "                mentions0.append(em)\n",
    "        \n",
    "        # put it all into splitText and mentions in the right way\n",
    "        splitText = []\n",
    "        mentions = []\n",
    "        curT = 0 # token index\n",
    "        curM = 0 # mention index\n",
    "        while(curT < len(tokens)):\n",
    "            if curM < len(mentions0) and curT == mentions0[curM]['docTokenBegin']:\n",
    "                # put in entity mention\n",
    "                splitText.append(mentions0[curM]['text'])\n",
    "                mentions.append([len(splitText) - 1,\n",
    "                                 mentions0[curM]['characterOffsetBegin'], \n",
    "                                 mentions0[curM]['characterOffsetEnd']])\n",
    "                curT = mentions0[curM]['docTokenEnd']\n",
    "                curM += 1\n",
    "                continue\n",
    "            else:\n",
    "                # put in next token\n",
    "                splitText.append(tokens[curT])\n",
    "                curT += 1\n",
    "        \n",
    "    elif mthd == 'cls1': # this one lest solr deal with overlaps\n",
    "        addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "        params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "        try:\n",
    "            tmp1 = text.encode('utf-8')\n",
    "        except:\n",
    "            tmp1 = text\n",
    "        r = requests.post(addr, params=params, data=tmp1)\n",
    "        textData0 = r.json()['tags']\n",
    "        splitText = [] # the text now in split form\n",
    "        mentions = [] # mentions before remove inadequate ones\n",
    "        textData = [] # [[begin,end,word,anchorProb],...]\n",
    "        i = 0 # for wordIndex\n",
    "        # get rid of extra un-needed Solr data\n",
    "        for item in textData0:\n",
    "            mentions.append([i, item[1], item[3]])\n",
    "            i += 1\n",
    "            # also fill split text\n",
    "            splitText.append(text[item[1]:item[3]])\n",
    "        if 'gbc-er' not in mlModels:\n",
    "            mlModels['gbc-er'] = pickle.load(open(mlModelFiles['gbc-er'], 'rb'))\n",
    "        mentions = getGoodMentions(splitText, mentions, mlModels['gbc-er'])\n",
    "        \n",
    "    elif mthd == 'cls2': # this one we deal with overlaps\n",
    "        addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "        params={'overlaps':'ALL', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on'}\n",
    "        \n",
    "        try:\n",
    "            tmp1 = text.encode('utf-8')\n",
    "        except:\n",
    "            tmp1 = text\n",
    "        r = requests.post(addr, params=params, data=tmp1)\n",
    "        textData0 = r.json()['tags']\n",
    "        splitText = [] # the text now in split form\n",
    "        mentions = [] # mentions before remove inadequate ones\n",
    "        textData = [] # [[begin,end,word,anchorProb],...]\n",
    "        i = 0 # for wordIndex\n",
    "        # get rid of extra un-needed Solr data\n",
    "        for item in textData0:\n",
    "            mentions.append([i, item[1], item[3]])\n",
    "            i += 1\n",
    "            # also fill split text\n",
    "            splitText.append(text[item[1]:item[3]])\n",
    "        if 'gbc-er' not in mlModels:\n",
    "            mlModels['gbc-er'] = pickle.load(open(mlModelFiles['gbc-er'], 'rb'))\n",
    "        mentions = getGoodMentions(splitText, mentions, mlModels['gbc-er'], True)\n",
    "    \n",
    "    # filter out mentions\n",
    "#     filters = []\n",
    "#     with open('/users/cs/amaral/wikisim/wikification/mentions-filter.txt', 'r') as f:\n",
    "#         for line in f:\n",
    "#             filters.append(line.strip())\n",
    "            \n",
    "    goodMentions = []\n",
    "    for mention in mentions:\n",
    "            goodMentions.append(mention)\n",
    "    \n",
    "    return {'text':splitText, 'mentions':goodMentions}\n",
    "\n",
    "def getMentionsInSentence(textData, mainWord):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Finds all mentions that are in the same sentence as mainWord.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        mainWord: The index of the word that is in the wanted sentence\n",
    "    Return:\n",
    "        A list of mention texts that are in the same sentence as mainWord\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = nltk.sent_tokenize(\" \".join(textData['text']))\n",
    "    \n",
    "    # start and end of sentences (absolute)\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    mentionStrs = [] # the mentions\n",
    "    \n",
    "    curEnd = 0\n",
    "    for sent in sents:\n",
    "        curEnd += len(sent)\n",
    "        # if sentence ends after mention starts\n",
    "        if curEnd > mainWord[1]:\n",
    "            sEnd = curEnd\n",
    "            sStart = sEnd - len(sent)\n",
    "            mWIndex = textData['mentions'].index(mainWord) # index of mainWord\n",
    "            \n",
    "            # add every mention before main in sent to mentionsStr\n",
    "            for i in range(mWIndex-1, -1, -1):\n",
    "                if textData['mentions'][i][2] > sStart:\n",
    "                    mentionStrs.append(textData['text'][textData['mentions'][i][0]])\n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "            # add every mention after main in sent to mentionsStr\n",
    "            for i in range(mWIndex+1, len(textData['mentions'])):\n",
    "                if textData['mentions'][i][1] < sEnd:\n",
    "                    mentionStrs.append(textData['text'][textData['mentions'][i][0]])\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return \" \".join(mentionStrs).strip()\n",
    "\n",
    "def generateCandidates(textData, maxC, hybrid = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Generates up to maxC candidates for each possible mention word in phrase.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        maxC: The max amount of candidates to accept.\n",
    "        Hybrid: Whether to include best context fitting results too.\n",
    "    Return:\n",
    "        The top maxC candidates for each possible mention word in textData. Each \n",
    "        mentions has its candidates of the form: [(wikiId, popularity),...]\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    ctxC0 = 0 # the amount of candidates to fill from best context.\n",
    "    if hybrid == True:\n",
    "        popC = int(maxC/2) + 1 # get ceil\n",
    "        ctxC0 = maxC - popC\n",
    "    else:\n",
    "        popC = maxC\n",
    "    \n",
    "    for mention in textData['mentions']:\n",
    "        resultT = sorted(anchor2concept(textData['text'][mention[0]]), key = itemgetter(1), \n",
    "                          reverse = True)[:popC]\n",
    "        results = [list(item) for item in resultT]\n",
    "        \n",
    "        # get the right amount to fill with context \n",
    "        if len(results) < popC and hybrid == True:\n",
    "            # fill in rest with context\n",
    "            ctxC = maxC - len(results)\n",
    "        elif hybrid == True:\n",
    "            ctxC = ctxC0\n",
    "        else:\n",
    "            ctxC = 0\n",
    "            \n",
    "        # get some context results from solr\n",
    "        if ctxC > 0:\n",
    "            mentionStr = escapeStringSolr(textData['text'][mention[0]])\n",
    "            ctxStr = escapeStringSolr(getMentionsInSentence(textData, mention))\n",
    "            \n",
    "            strIds = ['-id:' +  str(res[0]) for res in results]\n",
    "            \n",
    "            # select all the docs from Solr with the best scores, highest first.\n",
    "            addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "\n",
    "            try:\n",
    "                tmp1 = mentionStr.encode('utf-8')\n",
    "            except:\n",
    "                tmp1 = mentionStr\n",
    "            \n",
    "            if len(ctxStr) >= 0:\n",
    "                params={'fl':'id', 'indent':'on', 'fq':\" \".join(strIds),\n",
    "                        'q':'title:(' + tmp1+')^5',\n",
    "                        'wt':'json', 'rows': str(ctxC)}\n",
    "            else:\n",
    "                \n",
    "                try:\n",
    "                    tmp2 = ctxStr.encode('utf-8')\n",
    "                except:\n",
    "                    tmp2 = ctxStr\n",
    "                \n",
    "                params={'fl':'id', 'indent':'on', 'fq':\" \".join(strIds),\n",
    "                        'q':'title:(' + tmp1 + ')^5'\n",
    "                        + ' text:(' + tmp2 + ')',\n",
    "                        'wt':'json', 'rows':str(ctxC)}\n",
    "            \n",
    "            r = requests.get(addr, params = params)\n",
    "            try:\n",
    "                if ('response' in r.json() \n",
    "                        and 'docs' in r.json()['response']\n",
    "                        and len(r.json()['response']['docs']) > 0):\n",
    "                    for doc in r.json()['response']['docs']:\n",
    "                        # get popularity of entity given the mention\n",
    "                        popularity = 0\n",
    "                        thingys = anchor2concept(textData['text'][mention[0]])\n",
    "                        for thingy in thingys:\n",
    "                            if thingy[0] == long(doc['id']):\n",
    "                                popularity = thingy[1]\n",
    "                                break\n",
    "                        \n",
    "                        results.append([long(doc['id']), popularity])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        candidates.append(results[:maxC]) # take up to maxC of the results\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def precision(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The precision: (# of correct entities)/(# of found entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(mySet)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def recall(truthSet, mySet):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of mySet against the truthSet.\n",
    "    Args:\n",
    "        truthSet: The 'right' answers for what the entities are. [[start,end,id],...]\n",
    "        mySet: My code's output for what it thinks the right entities are. [[start,end,id],...]\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(truthSet)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    truthIndex = 0\n",
    "    myIndex = 0\n",
    "    \n",
    "    while truthIndex < len(truthSet) and myIndex < len(mySet):\n",
    "        if mySet[myIndex][0] < truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][1] > truthSet[truthIndex][0]:\n",
    "                # overlap with mine behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine not even reach truth\n",
    "                myIndex += 1\n",
    "                \n",
    "        elif mySet[myIndex][0] == truthSet[truthIndex][0]:\n",
    "            # same mention (same start atleast)\n",
    "            if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                numCorrect += 1\n",
    "                truthIndex += 1\n",
    "                myIndex += 1\n",
    "            elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                # truth ends first\n",
    "                truthIndex += 1\n",
    "            else:\n",
    "                # mine ends first\n",
    "                myIndex += 1\n",
    "                  \n",
    "        elif mySet[myIndex][0] > truthSet[truthIndex][0]:\n",
    "            if mySet[myIndex][0] < truthSet[truthIndex][1]:\n",
    "                # overlap with truth behind\n",
    "                if truthSet[truthIndex][2] == mySet[myIndex][2]:\n",
    "                    numCorrect += 1\n",
    "                    truthIndex += 1\n",
    "                    myIndex += 1\n",
    "                elif truthSet[truthIndex][1] < mySet[myIndex][1]:\n",
    "                    # truth ends first\n",
    "                    truthIndex += 1\n",
    "                else:\n",
    "                    # mine ends first\n",
    "                    myIndex += 1\n",
    "            else:\n",
    "                # mine beyond mention, increment truth\n",
    "                truthIndex += 1\n",
    "                \n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def mentionPrecision(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the precision of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The precision: (# of correct mentions)/(# of found mentions)\n",
    "    \"\"\"\n",
    "    \n",
    "    numFound = len(otherMentions)\n",
    "    numCorrect = 0 # incremented in for loop\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            #print ('MATCH: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <===> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            #print ('FAIL: [' + str(trueMentions[trueIndex][0]) + ',' + str(trueMentions[trueIndex][1]) + ']' + trueMentions[trueIndex][2] \n",
    "            #       + ' <XXX> [' + str(otherMentions[otherIndex][0]) + ',' + str(otherMentions[otherIndex][1]) + ']' + otherMentions[otherIndex][2])\n",
    "            otherIndex += 1\n",
    "        else:\n",
    "            print 'AAAAAAAHHHHHHHHHHHHHHHHHHHHHHHHHHHHH!!!!!!!!!!!!!!!!!!!'\n",
    "\n",
    "    #print 'correct: ' + str(numCorrect) + '\\nfound: ' + str(numFound)\n",
    "    if numFound == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numFound)\n",
    "\n",
    "def mentionRecall(trueMentions, otherMentions):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Calculates the recall of otherMentions against the trueMentions.\n",
    "    Args:\n",
    "        trueMentions: The 'right' answers for what the mentions are.\n",
    "        otherMentions: Our mentions obtained through some means.\n",
    "    Return:\n",
    "        The recall: (# of correct entities)/(# of actual entities)\n",
    "    \"\"\"\n",
    "    \n",
    "    numActual = len(trueMentions)\n",
    "    numCorrect = 0 # incremented in for loop)\n",
    "    \n",
    "    trueIndex = 0\n",
    "    otherIndex = 0\n",
    "    \n",
    "    while trueIndex < len(trueMentions) and otherIndex < len(otherMentions):\n",
    "        # if mentions start and end on the same\n",
    "        if (trueMentions[trueIndex][0] == otherMentions[otherIndex][0]\n",
    "               and trueMentions[trueIndex][1] == otherMentions[otherIndex][1]):\n",
    "            numCorrect += 1\n",
    "            trueIndex += 1\n",
    "            otherIndex += 1\n",
    "        # if true mention starts before the other starts\n",
    "        elif trueMentions[trueIndex][0] < otherMentions[otherIndex][0]:\n",
    "            trueIndex += 1\n",
    "        # if other mention starts before the true starts (same doesnt matter)\n",
    "        elif trueMentions[trueIndex][0] >= otherMentions[otherIndex][0]:\n",
    "            otherIndex += 1\n",
    "        \n",
    "    print 'correct: ' + str(numCorrect) + '\\nactual: ' + str(numActual)\n",
    "    if numActual == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (numCorrect/numActual)\n",
    "    \n",
    "def getSurroundingWords(text, mIndex, window, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the words surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    Args:\n",
    "        text: A list of words.\n",
    "        mIndex: The index of the word that is the center of where to get surrounding words.\n",
    "        window: The amount of words to the left and right to get.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The words that surround the given mention. Expanding out window elements\n",
    "        on both sides.\n",
    "    \"\"\"\n",
    "    \n",
    "    imin = mIndex - window\n",
    "    imax = mIndex + window + 1\n",
    "    \n",
    "    # fix extreme bounds\n",
    "    if imin < 0:\n",
    "        imin = 0\n",
    "    if imax > len(text):\n",
    "        imax = len(text)\n",
    "        \n",
    "    if asList == True:\n",
    "        words = (text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    else:\n",
    "        words = \" \".join(text[imin:mIndex] + text[mIndex+1:imax])\n",
    "    \n",
    "    # return surrounding part of word minus the mIndex word\n",
    "    return words\n",
    "\n",
    "def getMentionSentence(text, mention, asList = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the sentence of the mention, minus the mention.\n",
    "    Args:\n",
    "        text: The text to get the sentence from.\n",
    "        index: The mention.\n",
    "        asList: Whether to return the words as a list, otherwise just a string.\n",
    "    Return:\n",
    "        The sentence of the mention, minus the mention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the start and end indexes of the sentence\n",
    "    sStart = 0\n",
    "    sEnd = 0\n",
    "    \n",
    "    # get sentences using nltk\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # find sentence that mention is in\n",
    "    curLen = 0\n",
    "    for s in sents:\n",
    "        curLen += len(s)\n",
    "        # if greater than begin of mention\n",
    "        if curLen > mention[1]:\n",
    "            # remove mention from string to not get bias from self referencing article\n",
    "            if asList == True:\n",
    "                sentence = (s.replace(text[mention[1]:mention[2]],\"\")).split(\" \")\n",
    "            else:\n",
    "                sentence = s.replace(text[mention[1]:mention[2]],\"\")\n",
    "            \n",
    "            return sentence\n",
    "        \n",
    "    # in case it missed\n",
    "    if asList == True:\n",
    "        return []\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def escapeStringSolr(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Escapes a given string for use in Solr.\n",
    "    Args:\n",
    "        text: The string to escape.\n",
    "    Return:\n",
    "        The escaped text.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.replace(\"\\\\\", \"\\\\\\\\\\\\\")\n",
    "    text = text.replace('+', r'\\+')\n",
    "    text = text.replace(\"-\", \"\\-\")\n",
    "    text = text.replace(\"&&\", \"\\&&\")\n",
    "    text = text.replace(\"||\", \"\\||\")\n",
    "    text = text.replace(\"!\", \"\\!\")\n",
    "    text = text.replace(\"(\", \"\\(\")\n",
    "    text = text.replace(\")\", \"\\)\")\n",
    "    text = text.replace(\"{\", \"\\{\")\n",
    "    text = text.replace(\"}\", \"\\}\")\n",
    "    text = text.replace(\"[\", \"\\[\")\n",
    "    text = text.replace(\"]\", \"\\]\")\n",
    "    text = text.replace(\"^\", \"\\^\")\n",
    "    text = text.replace(\"\\\"\", \"\\\\\\\"\")\n",
    "    text = text.replace(\"~\", \"\\~\")\n",
    "    text = text.replace(\"*\", \"\\*\")\n",
    "    text = text.replace(\"?\", \"\\?\")\n",
    "    text = text.replace(\":\", \"\\:\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "def getContext1Scores(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    try:\n",
    "        tmp1 = context.encode('utf-8')\n",
    "    except:\n",
    "        tmp1 = context\n",
    "        \n",
    "    try:\n",
    "        tmp2 = mentionStr.encode('utf-8')\n",
    "    except:\n",
    "        tmp2 = mentionStr\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+tmp1+')^1 title:(' + tmp2+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    try:\n",
    "        # assign the scores\n",
    "        for doc in r.json()['response']['docs']:\n",
    "            # find candidate of doc\n",
    "            i = 0\n",
    "            for cand in candidates:\n",
    "                if cand[0] == long(doc['id']):\n",
    "                    candScores[i] = doc['score']\n",
    "                    break\n",
    "                i += 1\n",
    "    except:\n",
    "        # keep zero scores\n",
    "        pass\n",
    "            \n",
    "    return candScores\n",
    "\n",
    "def bestContext1Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['id:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    try:\n",
    "        tmp1 = context.encode('utf-8')\n",
    "    except:\n",
    "        tmp1 = context\n",
    "        \n",
    "    try:\n",
    "        tmp2 = mentionStr.encode('utf-8')\n",
    "    except:\n",
    "        tmp2 = mentionStr\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'id score', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'text:('+tmp1+')^1 title:(' + tmp2+')^1.35',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    bestId = long(r.json()['response']['docs'][0]['id'])\n",
    "    \n",
    "    #for doc in r.json()['response']['docs']:\n",
    "        #print '[' + id2title(doc['id']) + '] -> ' + str(doc['score'])\n",
    "    \n",
    "    # find which index has bestId\n",
    "    bestIndex = 0\n",
    "    for cand in candidates:\n",
    "        if cand[0] == bestId:\n",
    "            return bestIndex\n",
    "        else:\n",
    "            bestIndex += 1\n",
    "            \n",
    "    return bestIndex # in case it was missed\n",
    "\n",
    "def getContext2Scores(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    \n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "        \n",
    "    try:\n",
    "        tmp1 = context.encode('utf-8')\n",
    "    except:\n",
    "        tmp1 = context\n",
    "        \n",
    "    try:\n",
    "        tmp2 = mentionStr.encode('utf-8')\n",
    "    except:\n",
    "        tmp2 = mentionStr\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+tmp1+') entity:(' + tmp2 + ')^1',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    try:\n",
    "        # get count for each id\n",
    "        for doc in r.json()['response']['docs']:\n",
    "            scoreDict[str(doc['entityid'])] += 1\n",
    "    except:\n",
    "        # keep zero scores\n",
    "        pass\n",
    "    \n",
    "    # give scores to each cand\n",
    "    for j in range(0, len(candidates)):\n",
    "        candScores[j] = scoreDict[str(candidates[j][0])]\n",
    "            \n",
    "    return candScores\n",
    "\n",
    "def bestContext2Match(mentionStr, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the candidate that gives the highest relevance when given the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best relevance score from the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # put text in right format\n",
    "    context = escapeStringSolr(context)\n",
    "    mentionStr = escapeStringSolr(mentionStr)\n",
    "    strIds = ['entityid:' +  str(strId[0]) for strId in candidates]\n",
    "    \n",
    "    # dictionary to hold scores for each id\n",
    "    scoreDict = {}\n",
    "    for cand in candidates:\n",
    "        scoreDict[str(cand[0])] = 0\n",
    "        \n",
    "    try:\n",
    "        tmp1 = context.encode('utf-8')\n",
    "    except:\n",
    "        tmp1 = context\n",
    "        \n",
    "    try:\n",
    "        tmp2 = mentionStr.encode('utf-8')\n",
    "    except:\n",
    "        tmp2 = mentionStr\n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    params={'fl':'entityid', 'fq':\" \".join(strIds), 'indent':'on',\n",
    "            'q':'_context_:('+tmp1+') entity:(' + tmp2 + ')^1',\n",
    "            'wt':'json'}\n",
    "    r = requests.get(addr, params = params)\n",
    "    \n",
    "    if 'response' not in r.json():\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    if 'docs' not in r.json()['response']:\n",
    "        return 0\n",
    "    \n",
    "    results = r.json()['response']['docs']\n",
    "    if len(results) == 0:\n",
    "        return 0 # default to most popular\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        scoreDict[str(doc['entityid'])] += 1\n",
    "    \n",
    "    # get the index that has the best score\n",
    "    bestScore = 0\n",
    "    bestIndex = 0\n",
    "    curIndex = 0\n",
    "    for cand in candidates:\n",
    "        if scoreDict[str(cand[0])] > bestScore:\n",
    "            bestScore = scoreDict[str(cand[0])]\n",
    "            bestIndex = curIndex\n",
    "        curIndex += 1\n",
    "            \n",
    "    return bestIndex\n",
    "\n",
    "def getWord2VecScores(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the similarity scores of each mention to the context vector.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The scores of eac candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    candScores = []\n",
    "    for i in range(len(candidates)):\n",
    "        candScores.append(0)\n",
    "        \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        if math.isnan(score):\n",
    "            score = 0\n",
    "        candScores[i] = score\n",
    "        i += 1 # next index\n",
    "        \n",
    "    return candScores\n",
    "\n",
    "def bestWord2VecMatch(context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses word2vec to find the candidate with the best similarity to the context.\n",
    "    Args:\n",
    "        context: The words that surround the target word as a list.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The index of the candidate with the best similarity score with the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctxVec = pd.Series(sp.zeros(300)) # default zero vector\n",
    "    # add all context words together\n",
    "    for word in context:\n",
    "        ctxVec += getword2vector(word)\n",
    "        \n",
    "    # compare context vector to each of the candidates\n",
    "    bestIndex = 0\n",
    "    bestScore = 0\n",
    "    i = 0\n",
    "    for cand in candidates:\n",
    "        eVec = getentity2vector(str(cand[0]))\n",
    "        score = 1-sp.spatial.distance.cosine(ctxVec, eVec)\n",
    "        #print '[' + id2title(cand[0]) + ']' + ' -> ' + str(score)\n",
    "        # update score and index\n",
    "        if score > bestScore: \n",
    "            bestIndex = i\n",
    "            bestScore = score\n",
    "            \n",
    "        i += 1 # next index\n",
    "            \n",
    "    return bestIndex\n",
    "    \n",
    "def wikifyPopular(textData, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the most popular candidate for each mention.\n",
    "    Args:\n",
    "        textData: A text in split form along with its suspected mentions.\n",
    "        candidates: A list of list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][0][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "            \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyContext(textData, candidates, oText, useSentence = False, window = 7, method2 = False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidate that has the highest relevance with the surrounding window words.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window)\n",
    "            else:\n",
    "                #context = getMentionSentence(oText, mention)\n",
    "                context = getMentionsInSentence(textData, mention)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + context\n",
    "            if method2 == False:\n",
    "                bestIndex = bestContext1Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            else:\n",
    "                bestIndex = bestContext2Match(textData['text'][mention[0]], context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest similarity to the context.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text to be used for getting sentence.\n",
    "        useSentence: Whether to set use whole sentence as context, or just windowsize.\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCandidates = []\n",
    "    i = 0 # track which mention's candidates we are looking at\n",
    "    # for each mention choose the top candidate\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > 0:\n",
    "            if not useSentence:\n",
    "                context = getSurroundingWords(textData['text'], mention[0], window, asList = True)\n",
    "            else:\n",
    "                context = getMentionSentence(oText, mention, asList = True)\n",
    "            #print '\\nMention: ' + textData['text'][mention[0]]\n",
    "            #print 'Context: ' + \" \".join(context)\n",
    "            bestIndex = bestWord2VecMatch(context, candidates[i])\n",
    "            topCandidates.append([mention[1], mention[2], candidates[i][bestIndex][0]])\n",
    "        i += 1 # move to list of candidates for next mention\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyCoherence(textData, candidates, ws = 5):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Chooses the candidates that have the highest coherence according to rvs pagerank method.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        ws: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    topCands = [] # the top candidate from each candidate list\n",
    "    candsScores = coherence_scores_driver(candidates, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    i = -1 # track what mention we are on\n",
    "    for cScores in candsScores:\n",
    "        i += 1\n",
    "        \n",
    "        if len(cScores) == 0:\n",
    "            continue # nothing to do with this one\n",
    "            \n",
    "        bestScore = sorted(cScores, reverse = True)[0]\n",
    "        curIndex = 0\n",
    "        for score in cScores:\n",
    "            if score == bestScore:\n",
    "                topCands.append([textData['mentions'][i][1], textData['mentions'][i][2], candidates[i][curIndex][0]])\n",
    "                break\n",
    "            curIndex += 1\n",
    "            \n",
    "    return topCands\n",
    "\n",
    "mlModels = {} # dictionary of different models\n",
    "mlModelFiles = {\n",
    "    'abc': '/users/cs/amaral/wikisim/wikification/ml-models/model-abc-10000-hyb.pkl',\n",
    "    'bgc': '/users/cs/amaral/wikisim/wikification/ml-models/model-bgc-10000-hyb.pkl',\n",
    "    'etc': '/users/cs/amaral/wikisim/wikification/ml-models/model-etc-10000-hyb.pkl',\n",
    "    'gbc': '/users/cs/amaral/wikisim/wikification/ml-models/model-gbc-10000-hyb.pkl',\n",
    "    'rfc': '/users/cs/amaral/wikisim/wikification/ml-models/model-rfc-10000-hyb.pkl',\n",
    "    'lsvc': '/users/cs/amaral/wikisim/wikification/ml-models/model-lsvc-10000-hyb.pkl',\n",
    "    'svc': '/users/cs/amaral/wikisim/wikification/ml-models/model-svc-10000-hyb.pkl',\n",
    "    'lmart': '/users/cs/amaral/wikisim/wikification/ml-models/model-lmart-10000-pop-no-w2v.pkl',\n",
    "    'gbc-er': '/users/cs/amaral/wikisim/wikification/ml-models/er/er-model-gbc-30000.pkl',\n",
    "    'bgc-er': '/users/cs/amaral/wikisim/wikification/ml-models/er/er-model-bgc-30000.pkl'}\n",
    "\n",
    "def wikifyMulti(textData, candidates, oText, model, useSentence = True, window = 7):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Disambiguates each of the mentions with their given candidates using the desired\n",
    "        machine learned model.\n",
    "    Args:\n",
    "        textData: A textData in split form along with its suspected mentions.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "        oText: The original text, unsplit.\n",
    "        model: The machine learned model to use for disambiguation: \n",
    "            'gbc' (gradient boosted classifier), 'etr' (extra trees regression), \n",
    "            'gbr' (gradient boosted regression), 'lmart' (LambdaMART (a learning to rank method)),\n",
    "            and 'rfr' (random forest regression).\n",
    "        useSentence: Whether to use windo size of sentence (for context methods)\n",
    "        window: How many words on both sides of a mention to search for context.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    mlModel = mlModels[model] # get reference to model\n",
    "    \n",
    "    # get score from coherence\n",
    "    cohScores = coherence_scores_driver(candidates, 5, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    \n",
    "    i = 0\n",
    "    # get scores from each disambiguation method for all mentions\n",
    "    for mention in textData['mentions']:\n",
    "        if len(candidates[i]) > -1: # stub\n",
    "            # get the scores from each basic method.\n",
    "            \n",
    "            # normalize popularity scores\n",
    "            cScrs = []\n",
    "            for cand in candidates[i]:\n",
    "                cScrs.append(cand[1])\n",
    "            cScrs = normalize(cScrs)\n",
    "            j = 0\n",
    "            for cand in candidates[i]:\n",
    "                cand[1] = cScrs[j]\n",
    "                j += 1\n",
    "            \n",
    "            contextMInS = getMentionsInSentence(textData, textData['mentions'][i])\n",
    "            contextS = getMentionSentence(oText, textData['mentions'][i], asList = True)\n",
    "            \n",
    "            # context 1 scores\n",
    "            cScrs = getContext1Scores(textData['text'][mention[0]], contextMInS, candidates[i])\n",
    "            cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cScrs[j])\n",
    "            \n",
    "            # context 2 scores\n",
    "            cScrs = getContext2Scores(textData['text'][mention[0]], contextMInS, candidates[i])\n",
    "            cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cScrs[j])\n",
    "\n",
    "            # get score from word2vec\n",
    "            #cScrs = getWord2VecScores(contextS, candidates[i])\n",
    "            #cScrs = normalize(cScrs)\n",
    "            # apply score to candList\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                #candidates[i][j].append(cScrs[j])\n",
    "                candidates[i][j].append(0)\n",
    "\n",
    "            # get score from coherence\n",
    "            for j in range(0, len(candidates[i])):\n",
    "                candidates[i][j].append(cohScores[i][j])\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "    topCandidates = []\n",
    "    \n",
    "    i = 0\n",
    "    # go through all mentions again to disambiguate with ml model\n",
    "    for mention in textData['mentions']:\n",
    "        try:\n",
    "            Xs = [cand[1:] for cand in candidates[i]]\n",
    "            if len(Xs) == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            pred = mlModel.predict(Xs)\n",
    "        except:\n",
    "            try:\n",
    "                Xs = [cand[1:] for cand in candidates[i]]\n",
    "                pred = mlModel.predict(np.array(candidates[i][1:]).reshape(1, -1))\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        cur = 0\n",
    "        best = 0\n",
    "        bestI = 0\n",
    "        for j in range(len(pred)):\n",
    "            if pred[j] > best:\n",
    "                best = pred[j]\n",
    "                bestI = j\n",
    "        \n",
    "        topCandidates.append([mention[1], mention[2], candidates[i][bestI][0]])\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return topCandidates\n",
    "\n",
    "def wikifyEval(text, mentionsGiven, maxC = 20, method='popular', \n",
    "               strict = False, hybridC = True, model = 'lmart', erMethod = 'cls1'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes the text (maybe text data), and wikifies it for evaluation purposes using the desired method.\n",
    "    Args:\n",
    "        text: The string to wikify. Either as just the original string to be modified, or in the \n",
    "            form of: [[w1,w2,...], [[wid,entityId],...] if the mentions are given.\n",
    "        mentionsGiven: Whether the mentions are given to us and the text is already split.\n",
    "        maxC: The max amount of candidates to extract.\n",
    "        method: The method used to wikify.\n",
    "        strict: Whether to use such rules as minimum metion length, or minimum frequency of concept.\n",
    "        hybridC: Whether to split generated candidates between best of most frequent of most context related.\n",
    "        model: What model to use if using machine learning based method. LambdaMART as 'lmart' is default.\n",
    "            Other options are: 'gbc' (gradient boosted classifier), 'etr' (extra trees regression), \n",
    "            'gbr' (gradient boosted regression), and 'rfr' (random forest regression).\n",
    "        erMethod: The method to use for ER. 'cls1', 'cls2', 'cnlp'.\n",
    "    Return:\n",
    "        All of the proposed entities for the mentions, of the form: [[start,end,entityId],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    if not(mentionsGiven): # if words are not in pre-split form\n",
    "        text = text.replace(u'\\u2010', '-')\n",
    "        text = text.replace(u'\\u2011', '-')\n",
    "        text = text.replace(u'\\u2012', '-')\n",
    "        text = text.replace(u'\\u2013', '-')\n",
    "        text = text.replace(u'\\u2014', '-')\n",
    "        text = text.replace(u'\\u2015', '-')\n",
    "        textData = mentionExtract(text, mthd = erMethod) # extract mentions from text\n",
    "        oText = text # the original text\n",
    "    else: # if they are\n",
    "        textData = text\n",
    "        textData['mentions'] = mentionStartsAndEnds(textData) # put mentions in right form\n",
    "        oText = \" \".join(text['text'])\n",
    "    \n",
    "    # get rid of small mentions\n",
    "    if strict:\n",
    "        textData['mentions'] = [item for item in textData['mentions']\n",
    "                    if  len(textData['text'][item[0]]) >= MIN_MENTION_LENGTH]\n",
    "    \n",
    "    if method == 'popular':\n",
    "        maxC = 1 # only need one cand for popular\n",
    "    \n",
    "    candidates = generateCandidates(textData, maxC, hybridC)\n",
    "    \n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context1':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7)\n",
    "    elif method == 'context2':\n",
    "        wikified = wikifyContext(textData, candidates, oText, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        wikified = wikifyWord2Vec(textData, candidates, oText, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    elif method == 'multi':\n",
    "        if model not in mlModels:\n",
    "            mlModels[model] = pickle.load(open(mlModelFiles[model], 'rb'))\n",
    "        wikified = wikifyMulti(textData, candidates, oText, model, useSentence = True, window = 7)\n",
    "    \n",
    "    # get rid of very unpopular mentions\n",
    "    if strict:\n",
    "        wikified = [item for item in wikified\n",
    "                    if item[3] >= MIN_FREQUENCY]\n",
    "    \n",
    "    return wikified\n",
    "\n",
    "def doWikify(text, maxC = 20, hybridC = False, method = 'multi', erMethod = 'cls2'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Takes in text, and returns the location of mentions as well as the\n",
    "        entities they refer to.\n",
    "    Args:\n",
    "        text: The text to be wikified.\n",
    "    Return:\n",
    "        A list of mentions where each element contains the character offset\n",
    "        start and end, as well as the corresponding wikipedia page.\n",
    "    \"\"\"\n",
    "    # find the mentions\n",
    "    # text data now has text in split form and, the mentions\n",
    "    textData = mentionExtract(text, mthd = erMethod)\n",
    "    \n",
    "    \n",
    "    # generate candidates\n",
    "    candidates = generateCandidates(textData, maxC, hybridC)\n",
    "    \n",
    "    # disambiguate each mention to its candidates\n",
    "    if method == 'popular':\n",
    "        wikified = wikifyPopular(textData, candidates)\n",
    "    elif method == 'context1':\n",
    "        wikified = wikifyContext(textData, candidates, text, useSentence = True, window = 7)\n",
    "    elif method == 'context2':\n",
    "        wikified = wikifyContext(textData, candidates, text, useSentence = True, window = 7, method2 = True)\n",
    "    elif method == 'word2vec':\n",
    "        try:\n",
    "            word2vec\n",
    "        except:\n",
    "            word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "        wikified = wikifyWord2Vec(textData, candidates, text, useSentence = False, window = 5)\n",
    "    elif method == 'coherence':\n",
    "        wikified = wikifyCoherence(textData, candidates, ws = 5)\n",
    "    elif method == 'multi':\n",
    "        try:\n",
    "            #word2vec\n",
    "            pass\n",
    "        except:\n",
    "            #word2vec = gensim_loadmodel('/users/cs/amaral/cgmdir/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "            pass\n",
    "        if 'lmart' not in mlModels:\n",
    "            mlModels['lmart'] = pickle.load(open(mlModelFiles['lmart'], 'rb'))\n",
    "        wikified = wikifyMulti(textData, candidates, text, 'lmart', useSentence = True, window = 7)\n",
    "    \n",
    "    return wikified\n",
    "\n",
    "def annotateText(text, maxC = 20, hybridC = False, method = 'multi', erMethod = 'cls2'):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Annotates text with html anchor tags linking to the wikipedia pages\n",
    "        of the suspected entities.\n",
    "    Args:\n",
    "        text: The text to be annotated.\n",
    "    Return:\n",
    "        The text where the mentions are in anchor tags that link to the \n",
    "        corresponding wikipedia page.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the annotations\n",
    "    ants = doWidokify(text, maxC = maxC, hybridC = hybridC, method = method, erMethod = erMethod)\n",
    "    \n",
    "    # get title and intro of each entity\n",
    "    strIds = ['id:' +  str(ant[2]) for ant in ants]\n",
    "    addr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'fl':'title opening_text id', 'fq':\" \".join(strIds), \n",
    "            'indent':'on', 'q':'*:*', 'wt':'json', 'rows':str(len(ants))}\n",
    "    r = requests.get(addr, params = params)\n",
    "    try:\n",
    "        for doc in r.json()['response']['docs']:\n",
    "            # find ant with same id as doc\n",
    "            for ant in ants:\n",
    "                if len(ant) == 3 and str(ant[2]) == doc['id']:\n",
    "                    ant.extend([doc['title'], doc['opening_text'][:250] + '...'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # fill in all unfilled ants with thing\n",
    "    for ant in ants:\n",
    "        if len(ant) == 3:\n",
    "            ant.extend([id2title(ant[2]).replace('_', ' '), 'Description Not Found.'])\n",
    "            tmp = ant[3]\n",
    "            ant[3] = ''.join([i if ord(i) < 128 else '' for i in tmp]) # filter out non ascii, https://stackoverflow.com/questions/20078816/replace-non-ascii-characters-with-a-single-space\n",
    "    \n",
    "    newText = '' # the text to return with anchor tags\n",
    "    skip = 0\n",
    "    curM = 0 # cur mention index\n",
    "    for i in range(len(text)):\n",
    "        if skip > 0:\n",
    "            skip -= 1\n",
    "            continue\n",
    "        if curM < len(ants) and i == ants[curM][0]:\n",
    "            skip = ants[curM][1] - ants[curM][0] - 1\n",
    "            newText += ('<a class=\"toooltip\" target=\"_blank\" href=\"https://en.wikipedia.org/wiki/'\n",
    "                       + id2title(ants[curM][2]) + '\">' \n",
    "                       + text[ants[curM][0]:ants[curM][1]] \n",
    "                       + '<span class=\"toooltiptext\"><strong>' + ants[curM][3].encode('utf-8') + '</strong><br/>' \n",
    "                       + ants[curM][4].encode('utf-8')\n",
    "                       + '</span></a>')\n",
    "            curM += 1\n",
    "        else:\n",
    "            newText += text[i]\n",
    "            \n",
    "    return newText\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 12: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-44470aa4baad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'I go to  Pokémon Diamond'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtextData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentionExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmthd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cnlp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/raid6/user/sajadi/projects/wikisim/wikify/wikification.py\u001b[0m in \u001b[0;36mmentionExtract\u001b[0;34m(text, mthd)\u001b[0m\n\u001b[1;32m    472\u001b[0m         output = scnlp.annotate(text, properties={\n\u001b[1;32m    473\u001b[0m             \u001b[0;34m'annotators'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'entitymentions'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0;34m'outputFormat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         })\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/grad/sajadi/backup/anaconda2/envs/wikisim/lib/python2.7/site-packages/pycorenlp/corenlp.pyc\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, properties)\u001b[0m\n\u001b[1;32m     23\u001b[0m             '$ java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer')\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         r = requests.post(\n\u001b[1;32m     27\u001b[0m             self.server_url, params={\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc3 in position 12: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "from wikification import *\n",
    "from IPython.core.display import display, HTML\n",
    "text='I go to  Pokémon Diamond'\n",
    "textData = mentionExtract(text, mthd = 'cnlp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mentions': [[3, 8, 17]], 'text': [u'I', u'go', u'to', u'Dalhousie']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wikification import annotateText\n",
    "from IPython.core.display import display, HTML\n",
    "#print annotateText(\"The points classification was decided during the final stage and was won by Alejandro Valverde (Movistar Team), while Rodriguez won the combination classification. The mountains classification was won by Omar Fraile (Caja Rural-Seguros RGA). Dumoulin won the combativity award, while Movistar won the team prize.\")\n",
    "display(\n",
    " HTML(\n",
    "  annotateText(\"I am currently an undergrad computer science student at Dalhousie University who holds a diploma in IT from NSCC. At NSCC I focused on programming and database development. At Dalhousie I am focusing on artificial intelligence (a specialization option for computer science students), and math (my minor).\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikification import wikifyEval\n",
    "from wikipedia import id2title, anchor2concept\n",
    "\n",
    "the = wikifyEval('Ryan Amaral is a student at Dalhousie University in Halifax, Nova Scotia.', False, method = 'context2', hybridC = False)\n",
    "print the\n",
    "#for thing in the:\n",
    "#    print id2title(thing[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile word-pos-data.py\n",
    "from __future__ import division\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "Gets stats on the POS tag data of mentions and non-mentions.\n",
    "\"\"\"\n",
    "\n",
    "pathStrt = '/users/cs/amaral/wsd-datasets'\n",
    "dsPath = os.path.join(pathStrt,'wiki-mentions.30000.json')\n",
    "\n",
    "with open(dsPath, 'r') as dataFile:\n",
    "    dataLines = []\n",
    "    skip = 0\n",
    "    amount = 30000 # do 30000 for full\n",
    "    i = 0\n",
    "    for line in dataFile:\n",
    "        if i >= skip:\n",
    "            dataLines.append(json.loads(line.decode('utf-8').strip()))\n",
    "        i += 1\n",
    "        if i >= skip + amount:\n",
    "            break\n",
    "            \n",
    "mentionB = {}\n",
    "mentionC = {}\n",
    "mentionA = {}\n",
    "mentionBA = {}\n",
    "\n",
    "nonmentionB = {}\n",
    "nonmentionC = {}\n",
    "nonmentionA = {}\n",
    "nonmentionBA = {}\n",
    "\n",
    "mentions = 0\n",
    "nonmentions = 0\n",
    "\n",
    "lnum = 0\n",
    "for line in dataLines:\n",
    "    lnum += 1\n",
    "    print 'Line: ' + str(lnum)\n",
    "    \n",
    "    pos = nltk.pos_tag(line['text'])\n",
    "    for i in range(len(line['text'])):\n",
    "        # before\n",
    "        if i == 0:\n",
    "            keyB = 'NONE'\n",
    "        else:\n",
    "            keyB = pos[i-1][1]\n",
    "            \n",
    "        # current\n",
    "        keyC = pos[i][1]\n",
    "        \n",
    "        # after\n",
    "        if i == len(line['text']) - 1:\n",
    "            keyA = 'NONE'\n",
    "        else:\n",
    "            keyA = pos[i+1][1]\n",
    "        \n",
    "        if i in [mnt[0] for mnt in line['mentions']]: # is mention\n",
    "            mentions += 1\n",
    "            # before\n",
    "            try:\n",
    "                mentionB[keyB][0] += 1\n",
    "            except:\n",
    "                mentionB[keyB] = [1]\n",
    "            # current\n",
    "            try:\n",
    "                mentionC[keyC][0] += 1\n",
    "            except:\n",
    "                mentionC[keyC] = [1]\n",
    "            # after\n",
    "            try:\n",
    "                mentionA[keyA][0] += 1\n",
    "            except:\n",
    "                mentionA[keyA] = [1]\n",
    "            # before and after\n",
    "            try:\n",
    "                mentionBA[keyB + ' : ' + keyA][0] += 1\n",
    "            except:\n",
    "                mentionBA[keyB + ' : ' + keyA] = [1]\n",
    "        else: # is nonmention\n",
    "            nonmentions += 1\n",
    "            # before\n",
    "            try:\n",
    "                nonmentionB[keyB][0] += 1\n",
    "            except:\n",
    "                nonmentionB[keyB] = [1]\n",
    "            # current\n",
    "            try:\n",
    "                nonmentionC[keyC][0] += 1\n",
    "            except:\n",
    "                nonmentionC[keyC] = [1]\n",
    "            # after\n",
    "            try:\n",
    "                nonmentionA[keyA][0] += 1\n",
    "            except:\n",
    "                nonmentionA[keyA] = [1]\n",
    "            # before and after\n",
    "            try:\n",
    "                nonmentionBA[keyB + ' : ' + keyA][0] += 1\n",
    "            except:\n",
    "                nonmentionBA[keyB + ' : ' + keyA] = [1]\n",
    "                \n",
    "print 'Mentions', mentions\n",
    "print 'Non-Mentions', nonmentions\n",
    "                \n",
    "# apply portion to each pos tag (mentions)\n",
    "for key in mentionB.keys():\n",
    "    mentionB[key].append(mentionB[key][0]/mentions)\n",
    "for key in mentionC.keys():\n",
    "    mentionC[key].append(mentionC[key][0]/mentions)\n",
    "for key in mentionA.keys():\n",
    "    mentionA[key].append(mentionA[key][0]/mentions)\n",
    "for key in mentionBA.keys():\n",
    "    mentionBA[key].append(mentionBA[key][0]/mentions)\n",
    "# apply portion to each pos tag (nonmentions)\n",
    "for key in nonmentionB.keys():\n",
    "    nonmentionB[key].append(nonmentionB[key][0]/nonmentions)\n",
    "for key in nonmentionC.keys():\n",
    "    nonmentionC[key].append(nonmentionC[key][0]/nonmentions)\n",
    "for key in nonmentionA.keys():\n",
    "    nonmentionA[key].append(nonmentionA[key][0]/nonmentions)\n",
    "for key in nonmentionBA.keys():\n",
    "    nonmentionBA[key].append(nonmentionBA[key][0]/nonmentions)\n",
    "\n",
    "\n",
    "\"\"\" Already have this data\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-bef.tsv', 'w') as f:\n",
    "    for key in mentionB.keys():\n",
    "        f.write(key + '\\t' + str(mentionB[key][0]) + '\\t' + str(mentionB[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-cur.tsv', 'w') as f:\n",
    "    for key in mentionC.keys():\n",
    "        f.write(key + '\\t' + str(mentionC[key][0]) + '\\t' + str(mentionC[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-aft.tsv', 'w') as f:\n",
    "    for key in mentionA.keys():\n",
    "        f.write(key + '\\t' + str(mentionA[key][0]) + '\\t' + str(mentionA[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-bef.tsv', 'w') as f:\n",
    "    for key in nonmentionB.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionB[key][0]) + '\\t' + str(nonmentionB[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-cur.tsv', 'w') as f:\n",
    "    for key in nonmentionC.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionC[key][0]) + '\\t' + str(nonmentionC[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-aft.tsv', 'w') as f:\n",
    "    for key in nonmentionA.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionA[key][0]) + '\\t' + str(nonmentionA[key][1]) + '\\n')\n",
    "\"\"\"\n",
    "\n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-mention-befaft.tsv', 'w') as f:\n",
    "    for key in mentionBA.keys():\n",
    "        f.write(key + '\\t' + str(mentionBA[key][0]) + '\\t' + str(mentionBA[key][1]) + '\\n')\n",
    "        \n",
    "with open('/users/cs/amaral/wikisim/wikification/pos-data/pos-nonmention-befaft.tsv', 'w') as f:\n",
    "    for key in nonmentionBA.keys():\n",
    "        f.write(key + '\\t' + str(nonmentionBA[key][0]) + '\\t' + str(nonmentionBA[key][1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
