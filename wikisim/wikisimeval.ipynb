{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook\n",
    "This notebook provided some methods to do evaluate Wikisim on different datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%system\n",
    "\n",
    "mysql -u root -pemilios -e 'set global key_buffer_size=4*1024*1024*1024;'\n",
    "mysql -u root -pemilios -e 'set global bulk_insert_buffer_size=1024*1024*1024;'\n",
    "mysql -u root -pemilios -e 'set global query_cache_size = 4*1024*1024*1024;'\n",
    "mysql -u root -pemilios -e 'set global query_cache_limit = 4*1024*1024*1024;'\n",
    "mysql -u root -pemilios -e 'set global tmp_table_size = 4*1024*1024*1024;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py \n",
    "\"\"\" Config file.\n",
    "\"\"\"\n",
    "#%load_ext autoreload\n",
    "#%autoreload\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "import datetime\n",
    "#import logging\n",
    "from wikipedia import *\n",
    "\n",
    "\n",
    "home = os.path.expanduser(\"~\");\n",
    "dsdir = os.path.join(\"../datasets/similarity\");\n",
    "#dsdir = os.path.join(home ,\"backup/projects/wikisim/datasets/similarity.orig\");\n",
    "workingdir = os.path.join(home , 'backup/tmp/');\n",
    "baseresdir = path = os.path.join(workingdir, 'results')\n",
    "\n",
    "\n",
    "#logging.basicConfig(filename=os.path.join(workingdir,'myapp.log'), level=logging.INFO);    \n",
    "\n",
    "    \n",
    "def resdir(hitsver, direction=None):   \n",
    "    path = os.path.join(baseresdir , graphtype(direction), hitsver);\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "    \n",
    "def tmpdir(direction, hitsver):\n",
    "    return os.path.join(resdir(hitsver, direction),'tmp/');\n",
    "\n",
    "def graphdir(direction):    \n",
    "    return os.path.join(getworkingdir , 'graphs' , wikisim.graphtypestr(direction));\n",
    "\n",
    "def initdirs(hitsver, direction):\n",
    "    path = resdir(hitsver, direction)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    path = tmpdir(direction, hitsver)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "def printflush(*str):\n",
    "    print str\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "def graphtype(direction):\n",
    "    if direction is None:\n",
    "        return ''\n",
    "    if direction == DIR_IN:\n",
    "        return 'in'\n",
    "    if direction == DIR_OUT:\n",
    "        return 'out'\n",
    "    if direction == DIR_BOTH:\n",
    "        return 'both' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluating rvspage rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing', 'MC_28-edited.csv')\n",
      "SpearmanrResult(correlation=0.88535817841839004, pvalue=3.8954352260417844e-10)\n",
      "0:00:00\n"
     ]
    }
   ],
   "source": [
    "#%%writefile rvseval.py \n",
    "\"\"\" Evaluating the method on Semantic Relatedness Datasets.\"\"\"\n",
    "\n",
    "#%load_ext autoreload\n",
    "#%autoreload\n",
    "\n",
    "import os\n",
    "import time;\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#%aimport wikipedia\n",
    "#%aimport calcsim\n",
    "from config import *\n",
    "from calcsim import *\n",
    "import gensim\n",
    "\n",
    "import functools\n",
    "\n",
    "#print DISABLE_CACHE\n",
    "#clearcache()\n",
    "direction=DIR_BOTH;\n",
    "#method = 'word2vec.300.orig'\n",
    "#method = 'word2vec.300.ehsan'\n",
    "#method = 'word2vec.graph.500'\n",
    "#method = 'word2vec.500'\n",
    "#method = 'word2vec.500.noneg'\n",
    "method = 'rvspagerank'\n",
    "#method = 'ngd'\n",
    "    \n",
    "#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "#word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5')\n",
    "#word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-cleantext.1.1.300.10.5.20.5.5/word2vec.enwiki-20160305-cleantext.1.1.300.10.5.20.5.5')\n",
    "\n",
    "\n",
    "#word2vec_path  = '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.20.0.15/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.20.0.15'\n",
    "#word2vec_path  = '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.15.5.5/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.15.5.5'\n",
    "#word2vec_path = '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.20.10.15/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.20.10.15'\n",
    "#word2vec_path = '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.25.20.20/word2vec.enwiki-20160305-pagelinks.dumped.shuf.1.0.500.2.5.25.20.20'\n",
    "\n",
    "\n",
    "\n",
    "initdirs(method, direction)\n",
    "resfilename =  os.path.join(baseresdir, 'reslog.txt')\n",
    "\n",
    "dsfiles=('MC_28-edited.csv', 'RG-edited.csv', 'wsim353-edited.csv', 'Kore-edited.csv', 'MiniMayoSRS-edited.csv', \n",
    "         'MayoSRS-edited.csv', 'UMNSRS_relatedness-edited.csv', 'UMNSRS_similarity-edited.csv')\n",
    "\n",
    "# dsfiles=('MC_28.orig.lower.csv', 'RG.orig.lower.csv', 'wsim353.orig.lower.csv', 'Kore-edited.csv','MiniMayoSRS.orig.lower.csv', \n",
    "#          'MayoSRS.orig.lower.csv', 'UMNSRS_relatedness.orig.lower.csv', 'UMNSRS_similarity.orig.lower.csv')\n",
    "\n",
    "dsfiles=('MC_28-edited.csv',)\n",
    "#dsfiles=('MC_28.orig.lower.csv',)\n",
    "\n",
    "if 'word2vec' in method:\n",
    "    wmodel = gensim_loadmodel(word2vec_path)\n",
    "    method += '.' + str(wmodel.negative)\n",
    "for dsname in dsfiles:\n",
    "    start = time.time()\n",
    "    \n",
    "    printflush (\"Processing\",dsname)\n",
    "    dsbase, dsext = os.path.splitext(dsname);\n",
    "    infilename = os.path.join(dsdir, dsname)\n",
    "    outfilename = os.path.join(resdir(method, direction), dsbase+ '.out'+ dsext)\n",
    "    _ , corr = getsim_file(infilename, outfilename, method, direction);\n",
    "    logres(resfilename, '%s\\t%s\\t%s\\t%s\\t%s', method, dsname, graphtype(direction), corr.correlation\n",
    "                        , corr.pvalue)\n",
    "    print corr\n",
    "    print str(timeformat(int(time.time()-start)));\n",
    "    \n",
    "#close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymethod='word2vec'\n",
    "getsim(encode_entity('David_Beckham',mymethod ),encode_entity('Madrid',mymethod),method, DIR_BOTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_entity('David_Beckham',mymethod )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile embeval.py \n",
    "\"\"\"Testing batch embedding for a given file. \"\"\"\n",
    "\n",
    "import os\n",
    "from scipy import stats\n",
    "import time;\n",
    "from config import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "%aimport calcsim\n",
    "from calcsim import *\n",
    "\n",
    "\n",
    "\n",
    "direction=DIR_OUT;\n",
    "initdirs('rvspagerank', direction)\n",
    "print \"starte\"\n",
    "dsfiles=('MiniMayoSRS-edited.csv','MayoSRS-edited.csv')\n",
    "start = time.time()\n",
    "for dsname in dsfiles:\n",
    "    printflush (\"Processing\",dsname)\n",
    "    dsbase, dsext = os.path.splitext(dsname);\n",
    "    infilename = os.path.join(dsdir, dsname)\n",
    "    outfilename = os.path.join(resdir('rvspagerank', direction), dsbase+ '.emb'+ dsext)\n",
    "    getembed_file(infilename, outfilename,direction, cutoff=10);\n",
    "    \n",
    "print str(timeformat(int(time.time()-start)));\n",
    "#close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Pre-embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import MySQLdb\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "\n",
    "from json import encoder\n",
    "\n",
    "#encoder.FLOAT_REPR = lambda o: format(o, '.2f') #Doesn't Work!\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "_cursor = _db.cursor()\n",
    "\n",
    "#dirstr = 'in'\n",
    "dirstr = 'out'\n",
    "emb_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.ssv')\n",
    "\n",
    "if dirstr == 'in': \n",
    "    tablename = 'pagelinksorderedin';\n",
    "elif dirstr == 'out': \n",
    "    tablename = 'pagelinksorderedout';\n",
    "\n",
    "\n",
    "_cursor.execute(\"\"\"SELECT * FROM `{0}`\"\"\".format(tablename))\n",
    "rows = _cursor.fetchall();\n",
    "\n",
    "with open(emb_fname, 'w') as emb_f:\n",
    "    for row in rows:\n",
    "        wid=str(row[0])\n",
    "        values, index = pickle.loads(row[1])\n",
    "        emb=json.dumps(dict(zip(index,values)))\n",
    "        emb_f.write(wid+\"\\t\"+emb+'\\n')\n",
    "\n",
    "emb_f.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of the dead embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from config import *\n",
    "\n",
    "dead_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.in.dead.ssv')\n",
    "\n",
    "data = pd.read_table(dead_fname, header=None, usecols=[2])\n",
    "data = data[data[2] != 0]\n",
    "\n",
    "\n",
    "dead_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.in.dead.ssv')\n",
    "\n",
    "bins = [0,3,4,5,6,7,8,9,10,20,30,40,50,100]\n",
    "bins=[b*10000 for b in bins]\n",
    "h = np.histogram(data,bins)\n",
    "print \"bins: \", h[1]\n",
    "print \"bined data: \", h[0]\n",
    "\n",
    "plt.hist(data, bins, histtype='bar')\n",
    "plt.title('size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# In-Out degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generating some wiki statistics. \"\"\"\n",
    "import operator\n",
    "import os\n",
    "from collections import defaultdict\n",
    "home = os.path.expanduser(\"~\");\n",
    "from wikipedia import *\n",
    "with open(home+\"/backup/projects/datasets/embed/allids.csv\") as f:\n",
    "    allids=set(line.strip() for line in f);\n",
    "outcount = defaultdict();    \n",
    "for id in allids:\n",
    "    if not id.isdigit(): continue;\n",
    "    outcount[id]=len(getlinkedpages(id,DIR_IN));\n",
    "#o=w.getlinkedpages_query('None',Wikipedia.DIR_OUT);\n",
    "#print o;\n",
    "#w.cursor.execute(o)\n",
    "#s=w.getlinkedpages_query(29953972,Wikipedia.DIR_OUT);\n",
    "#outcount_sorted=sorted(outcount.items(), key=operator.itemgetter(1))\n",
    "print outcount_sorted\n",
    "close();    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
