{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Semantic Relatedness using Wikipedia\n",
    "\n",
    "* **Armin Sajadi** (sajadi@cs.dal.ca)\n",
    "\n",
    "This is a simple and step by step explanation of calculating semantic relatedness using Wikipedia. We start by preprocessing and building the api, that is explained in the following papers papers:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Here First\n",
    "** Make sure you have already followed [the first 2 Steps](../README.md#hosting) of the setup and have all the requirements before trying to setup to run these scripts **\n",
    "\n",
    "\n",
    "# Table of Context\n",
    "\n",
    "**[Wikipedia Interface](#Wikipedia-Interface)**\n",
    "\n",
    "**[Fast Pagerank Implementation](#Fast-Pagerank-Implementation)**\n",
    "\n",
    "**[Calculating Semantic Relatedness](#Calculating-Semantic-Relatedness)**\n",
    "\n",
    "**[A Simple Example](#A-Simple-Example)**\n",
    "\n",
    "**[Calculating All The Embeddngs](#Calculating-All-The-Embeddngs)**\n",
    "\n",
    "**[Visualizing The Embeddings](#Visualizing-The-Embeddings)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Interface\n",
    "This is the main interface to Wikipedia database and provides basic functions given a pages, such as its:\n",
    "\n",
    "* id or title\n",
    "* synonym ring\n",
    "* linkage\n",
    "* in or out neighborhood. \n",
    "\n",
    "**You might need to modify, user, password and portnumbers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikipedia.py \n",
    "\"\"\"A General Class to interact with Wiki datasets\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import sys;\n",
    "import os\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import MySQLdb\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "DISABLE_CACHE=False;\n",
    "MAX_GRAPH_SIZE=1000000\n",
    "\n",
    "DIR_IN=0;\n",
    "DIR_OUT=1;\n",
    "DIR_BOTH=2;\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "_cursor = _db.cursor()\n",
    "#WIKI_SIZE = 10216236;\n",
    "#WIKI_SIZE = 13670498; #2016\n",
    "WIKI_SIZE = 5576365; #no redirect, 2016\n",
    "def close():\n",
    "    global _db, _cursor;\n",
    "    if _cursor is not None: \n",
    "        _cursor.close();\n",
    "        _db.close();\n",
    "    _cursor=_db=None;\n",
    "def reopen():\n",
    "    global _db, _cursor;\n",
    "    if _db is None:\n",
    "        _db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "        _cursor = _db.cursor()\n",
    "\n",
    "def disable_cache():\n",
    "    global DISABLE_CACHE\n",
    "    DISABLE_CACHE=True\n",
    "def enable_cache():\n",
    "    global DISABLE_CACHE\n",
    "    DISABLE_CACHE=False\n",
    "        \n",
    "def load_table(tbname, limit=-1):\n",
    "    \"\"\" Returns a list, containing a whole table     \n",
    "    \n",
    "    Args: \n",
    "        tbname: Table Name\n",
    "    Returns: \n",
    "        The list of rows\n",
    "    \"\"\"\n",
    "    if limit!=-1:\n",
    "        q = \"\"\"SELECT * FROM `%s` limit %s\"\"\" % (tbname, limit)\n",
    "    else:\n",
    "        q = \"\"\"SELECT * FROM `%s`\"\"\" % (tbname,)\n",
    "        \n",
    "    _cursor.execute(q)\n",
    "    rows = _cursor.fetchall();\n",
    "    return rows\n",
    "    \n",
    "    \n",
    "def id2title(wid):\n",
    "    \"\"\" Returns the title for a given id\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id       \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"\n",
    "    title=None;\n",
    "\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_id = %s\"\"\", (wid,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        title=row[2];          \n",
    "    return title;\n",
    "\n",
    "def ids2title(wids):\n",
    "    \"\"\" Returns the titles for given list of wikipedia ids \n",
    "\n",
    "    Args: \n",
    "        wids: A list of Wikipedia ids          \n",
    "    Returns: \n",
    "        The list of titles\n",
    "    \"\"\"\n",
    "    if not wids:\n",
    "        return []\n",
    "    wid_list = [str(wid) for wid in wids] ;\n",
    "    order = ','.join(['page_id'] + wid_list) ;\n",
    "    wid_str = \",\".join(wid_list)\n",
    "    query = \"SELECT page_id, page_title FROM `page` where page_id in ({0})\" \\\n",
    "    .format(wid_str, order);\n",
    "    _cursor.execute(query);\n",
    "    rows = _cursor.fetchall();\n",
    "    rows_dict = defaultdict(lambda: None, rows)\n",
    "    titles = [rows_dict[wid] for wid in wids]\n",
    "    return titles;\n",
    "\n",
    "def encode_for_db(instr):\n",
    "    if isinstance(instr, unicode):\n",
    "        instr = instr.encode('utf-8')  \n",
    "    return instr\n",
    "        \n",
    "def normalize_str(title):\n",
    "    \n",
    "    title = encode_for_db(title)\n",
    "    title = title.replace(' ','_')\n",
    "    return title\n",
    "def title2id(title):\n",
    "    \"\"\" Returns the id for a given title\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id          \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"        \n",
    "    if title is None:\n",
    "        return None\n",
    "    wid=None;\n",
    "    title = normalize_str(title)\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_title=%s and page_namespace=0\"\"\", (title,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        wid = getredir_id(row[0]) if row[3] else row[0];\n",
    "    return wid;\n",
    "\n",
    "def is_ambiguous(wid):\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `categorylinks` WHERE `categorylinks`.cl_from=%s and `categorylinks`.cl_to=19204864;\"\"\", (wid,))\n",
    "    row= _cursor.fetchone();\n",
    "    return not (row is None)    \n",
    "\n",
    "def getredir_id(wid):\n",
    "    \"\"\" Returns the target of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The id of the target page\n",
    "    \"\"\"\n",
    "    rid=None\n",
    "\n",
    "    _cursor.execute(\"\"\"select * from redirect where rd_from=%s;\"\"\", (wid,));\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        rid=row[1]\n",
    "    return rid \n",
    "\n",
    "def resolveredir(wid):\n",
    "    tid = getredir_id(wid);\n",
    "    if tid is not None:\n",
    "        wid = tid;    \n",
    "    return wid\n",
    "\n",
    "def getredir_title(wid):\n",
    "    \"\"\" Returns the target title of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The title of the target page\n",
    "    \"\"\"\n",
    "    \n",
    "    title=None;\n",
    "    _cursor.execute(\"\"\" select page_title from redirect INNER JOIN page\n",
    "                  on redirect.rd_to = page.page_id \n",
    "                  where redirect.rd_from =%s;\"\"\", (wid));\n",
    "    row=_cursor.fetchone()\n",
    "    if row is not  None:\n",
    "        title=row[0];\n",
    "    return title;\n",
    "\n",
    "def synonymring_titles(wid):\n",
    "    \"\"\" Returns the synonim ring of a page\n",
    "\n",
    "    Example: synonymring_titles('USA')={('U.S.A', 'US', 'United_States_of_America', ...)}\n",
    "\n",
    "    Args:\n",
    "        wid: the wikipedia id\n",
    "    Returns:\n",
    "        all the titles in its synonym ring\n",
    "    \"\"\"\n",
    "    wid = resolveredir(wid)\n",
    "    _cursor.execute(\"\"\"(select page_title from page where page_id=%s) union \n",
    "                 (select page_title from redirect INNER JOIN page\n",
    "                    on redirect.rd_from = page.page_id \n",
    "                    where redirect.rd_to =%s);\"\"\", (wid,wid));\n",
    "    rows=_cursor.fetchall();\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows;\n",
    "\n",
    "\n",
    "def anchor2concept(anchor):\n",
    "    \"\"\" Returns the targets of an anchor text\n",
    "\n",
    "    Args:\n",
    "        anchor: anchor\n",
    "        \n",
    "    Returns:\n",
    "        The list of the titles of the linked pages\n",
    "    \"\"\"\n",
    "  \n",
    "    anchor = encode_for_db(anchor)\n",
    "        \n",
    "    _cursor.execute(\"\"\"select anchors.id, anchors.freq from anchors inner join page on anchors.id=page.page_id where anchors.anchor=%s;\"\"\", (anchor,))\n",
    "    rows =_cursor.fetchall()\n",
    "#     if rows:\n",
    "#         rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def id2anchor(wid):\n",
    "    \"\"\" Returns the targets of an anchor text\n",
    "\n",
    "    Args:\n",
    "        anchor: anchor\n",
    "        \n",
    "    Returns:\n",
    "        The list of the titles of the linked pages\n",
    "    \"\"\"\n",
    "    _cursor.execute(\"\"\"select anchor , freq from anchors where id=%s\"\"\", (wid,))\n",
    "    rows =_cursor.fetchall()\n",
    "#     if rows:\n",
    "#         rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _getlinkedpages_query(id, direction):\n",
    "    query=\"(SELECT {0} as lid FROM pagelinks where ({1} = {2}))\"\n",
    "    if direction == DIR_IN:\n",
    "        query=query.format(\"pl_from\",\"pl_to\",id);\n",
    "    elif direction == DIR_OUT:\n",
    "        query=query.format(\"pl_to\",\"pl_from\",id);\n",
    "    return query;\n",
    "\n",
    "def getlinkedpages(wid,direction):\n",
    "    \"\"\" Returns the linkage for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The list of the ids of the linked pages\n",
    "    \"\"\"\n",
    "    _cursor.execute(_getlinkedpages_query(wid, direction));\n",
    "    rows =_cursor.fetchall()\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "def e2i(wids):\n",
    "    elist=[];\n",
    "    edict=dict();\n",
    "    last=0;    \n",
    "    for wid in itertools.chain(*iters):\n",
    "        if wid not in edict:\n",
    "            edict[wid]=last;\n",
    "            elist.append(wid);\n",
    "            last +=1; \n",
    "    return elist, edict;\n",
    "\n",
    "def getneighbors(wid, direction):\n",
    "    \"\"\" Returns the neighborhood for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The vector of ids, and the 2d array sparse representation of the graph, in the form of\n",
    "        array([[row1,col1],[row2, col2]]). This form is flexible for general use or be converted to scipy.sparse \n",
    "        formats\n",
    "    \"\"\"\n",
    "    log('[getneighbors started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    idsquery = \"\"\"(select  {0} as lid) union {1}\"\"\".format(wid,_getlinkedpages_query(wid,direction));\n",
    "\n",
    "    _cursor.execute(idsquery);\n",
    "\n",
    "\n",
    "    rows = _cursor.fetchall();\n",
    "    if len(rows)<2:\n",
    "        log('[getneighbors]\\tERROR: empty')\n",
    "        return (), sp.array([])\n",
    "    \n",
    "    \n",
    "    neighids = tuple(r[0] for r in rows);\n",
    "    if len(neighids)>MAX_GRAPH_SIZE:\n",
    "        log('[getneighbors]\\tERROR: too big, %s neighbors', len(neighids))\n",
    "        return (), sp.array([])\n",
    "\n",
    "    \n",
    "    id2row = dict(zip(neighids, range(len(neighids))))\n",
    "\n",
    "    neighbquery=  \"\"\"select lid,pl_to as n_l_to from\n",
    "                     ({0}) a  inner join\n",
    "                     pagelinks on lid=pl_from\"\"\".format(idsquery);\n",
    "\n",
    "    links=_cursor.execute(neighbquery);\n",
    "\n",
    "    links = _cursor.fetchall();\n",
    "    \n",
    "    #links = tuple((id2row(u), id2row(v)) for u, v in links if (u in id2row) and (v in id2row));\n",
    "    links = sp.array([[id2row[u], id2row[v]] for u, v in links if (u in id2row) and (v in id2row)]);\n",
    "    \n",
    "    log('Graph extracted, %s nodes and %s linkes', len(neighids), len(links) )\n",
    "    log('[getneighbors]\\tfinished')\n",
    "    return (neighids,links)\n",
    "\n",
    "def deletefromcache(wid, direction):\n",
    "    wid = resolveredir(wid)\n",
    "    if direction in [DIR_IN, DIR_BOTH] : \n",
    "        query =    \"\"\"delete from {0} where cache_id={1}\"\"\".format('pagelinksorderedin', wid) \n",
    "        _cursor.execute(query);\n",
    "    if direction in [DIR_OUT, DIR_BOTH]: \n",
    "        query =    \"\"\"delete from {0} where cache_id={1}\"\"\".format('pagelinksorderedout', wid) \n",
    "        _cursor.execute(query);\n",
    "    \n",
    "def clearcache():\n",
    "    if DISABLE_CACHE:\n",
    "        return;\n",
    "    _cursor.execute(\"delete  from pagelinksorderedin\");\n",
    "    _cursor.execute(\"delete  from pagelinksorderedout\");\n",
    "\n",
    "def checkcache(wid, direction):\n",
    "    log('[checkcache started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[checkcache]\\tDisabled')\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "    em=None\n",
    "    \n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "    query =    \"\"\"select {0} from {1} where cache_id={2}\"\"\".format(colname, tablename, wid)\n",
    "    _cursor.execute(query);\n",
    "    row = _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        values, index = pickle.loads(row[0])\n",
    "        log('[checkcache]\\tfound')\n",
    "        if not index:        \n",
    "            log('[checkcache]\\tempty embedding')\n",
    "        em=pd.Series(values, index=index)\n",
    "    else:\n",
    "        log('[checkcache]\\tnot found')\n",
    "\n",
    "    log('[checkcache]\\tfinished')\n",
    "    return em\n",
    "\n",
    "\n",
    "def cachescores(wid, em, direction):\n",
    "    log('[cachescores started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[cachescores]\\tDisabled')\n",
    "        return\n",
    "\n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "        \n",
    "    idscstr = pickle.dumps((em.values.tolist(), em.index.values.tolist()), pickle.HIGHEST_PROTOCOL)\n",
    "    _cursor.execute(\"\"\"insert into %s values (%s,'%s');\"\"\" %(tablename, wid, _db.escape_string(idscstr)));\n",
    "    \n",
    "    \n",
    "    log('cachescores finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "Some small helper function for reporting purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile utils.py \n",
    "\"\"\"Utility functions\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "dirname = os.path.dirname(__file__)\n",
    "\n",
    "def readds(url, usecols=None):    \n",
    "    data = pd.read_table(url, header=None, usecols=usecols)\n",
    "    return data\n",
    "\n",
    "DISABLE_LOG=False;\n",
    "\n",
    "def clearlog(logfile):\n",
    "    with open(logfile, 'w'):\n",
    "        pass;\n",
    "\n",
    "def logres(outfile, instr, *params):\n",
    "    outstr = instr % params;\n",
    "    with open(outfile, 'a') as f:\n",
    "        f.write(\"[%s]\\t%s\\n\" % (str(datetime.datetime.now()) , outstr));          \n",
    "        \n",
    "def log(instr, *params):\n",
    "    if DISABLE_LOG:\n",
    "        return\n",
    "    logres(logfile, instr, *params)\n",
    "    \n",
    "outdir = os.path.join(dirname, '../out')    \n",
    "if not DISABLE_LOG:    \n",
    "    logfile=os.path.join(outdir, 'log.txt');\n",
    "    if not os.path.exists(logfile):\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        log('log created') \n",
    "        os.chmod(logfile, 0777)    \n",
    "    \n",
    "    \n",
    "def timeformat(sec):\n",
    "    return datetime.timedelta(seconds=sec)\n",
    "\n",
    "def str2delta(dstr):\n",
    "    r=re.match(('((?P<d>\\d+) day(s?), )?(?P<h>\\d+):(?P<m>\\d+):(?P<s>\\d*\\.\\d+|\\d+)'),dstr)\n",
    "    d,h,m,s=r.group('d'),r.group('h'),r.group('m'),r.group('s')\n",
    "    d=int(d) if d is not None else 0\n",
    "    h,m,s = int(h), int(m), float(s)    \n",
    "    return datetime.timedelta(days=d, hours=h, minutes=m, seconds=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Pagerank Implementation\n",
    "\n",
    "Here we have the actuall implementation of pagerank. Two implemenation are provided, both inspired  by the sparse fast solutions given in **Cleve Moler**'s book, [*Experiments with MATLAB*](http://www.mathworks.com/moler/index_ncm.html). The power method is much faster with enough precision for our task. Our benchmarsk shows that this implementation is faster than networkx implementation magnititude of times\n",
    "\n",
    "The input is a 2d array, each row of the array is an edge of the graph [[a,b], [c,d]], a and b are the node numbers. \n",
    "(In case you want to caclulate reall page rank, uncomment the line that transposes the adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pagerank.py \n",
    "\n",
    "\"\"\"Two \"fast\" implementations of PageRank.\n",
    "\n",
    "Pythom implementations of Matlab original in Cleve Moler, Experiments with MATLAB.\n",
    "\"\"\"\n",
    "# uncomment\n",
    "from __future__ import division\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg \n",
    "\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "def create_csr(Z):\n",
    "    \"\"\" Creates a csr presentation from 2darray presentation\n",
    "    Args:\n",
    "        Z: input graph in the form of a 2d array, such as sp.array([[2,0], [1,2], [2,1]])\n",
    "    Returns:\n",
    "        a csr representation\n",
    "    \n",
    "    \"\"\"   \n",
    "    rows = Z[:,0];\n",
    "    cols = Z[:,1];\n",
    "    n = max(max(rows), max(cols))+1;\n",
    "    G=sprs.csr_matrix((sp.ones(rows.shape),(rows,cols)), shape=(n,n));\n",
    "    return G\n",
    "\n",
    "def moler_pagerank_sparse(G, p=0.85, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    # In Moler's algorithm, $G_{ij}$ represents the existences of an edge\n",
    "    # from node $j$ to $i$, while we have assumed the opposite!\n",
    "    if not reverse: \n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        personalize=sp.ones(n) \n",
    "    personalize=personalize.reshape(n,1)\n",
    "    e=(personalize/personalize.sum())*n \n",
    "\n",
    "    I=sprs.eye(n)\n",
    "    x = sprs.linalg.spsolve((I - p*G.dot(D)), e);\n",
    "\n",
    "    x=x/x.sum()\n",
    "    return x\n",
    "def moler_pagerank_sparse_power(G, p=0.85, max_iter = 100,  tol=1e-03,personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        max_iter: maximum number of iterations\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    # In Moler's algorithm, $G_{ij}$ represents the existences of an edge\n",
    "    # from node $j$ to $i$, while we have assumed the opposite!\n",
    "    if not reverse: \n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        personalize=sp.ones(n) \n",
    "    personalize=personalize.reshape(n,1)\n",
    "    e=(personalize/personalize.sum())*n \n",
    "    \n",
    "    \n",
    "    z = (((1-p)*(c!=0) + (c==0))/n)[sp.newaxis,:]\n",
    "    G = p*G.dot(D)\n",
    "    \n",
    "    x = e/n \n",
    "    oldx = sp.zeros((n,1));\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while sp.linalg.norm(x-oldx) > tol:\n",
    "        oldx = x\n",
    "        x = G.dot(x) + e.dot(z.dot(x))\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break;\n",
    "    x = x/sum(x)\n",
    "    \n",
    "    return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile embedding.py\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from pagerank import * # uncomment\n",
    "import gensim\n",
    "\n",
    "#from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "_word2vec_model = None\n",
    "def getword2vec_model():\n",
    "    \"\"\" returns the word2vec model\n",
    "    \"\"\"\n",
    "    \n",
    "    return _word2vec_model\n",
    "\n",
    "def conceptrep(wid, method ='rvspagerank', direction=DIR_BOTH, get_titles=True, cutoff=None):\n",
    "    \"\"\" Calculates well-known similarity metrics between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "        method:\n",
    "            rvspagerank: rvs-pagerank embedding\n",
    "            word2vec : wor2vec representation\n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "        \n",
    "    if method =='rvspagerank':\n",
    "        return conceptrep_rvs(wid, direction, get_titles, cutoff)\n",
    "    if 'word2vec' in method:\n",
    "        return getentity2vector(wid)\n",
    "\n",
    "\n",
    "def concept_embedding(wid, direction):\n",
    "    \"\"\" Calculates concept embedding to be used in relatedness\n",
    "    \n",
    "    Args:\n",
    "        wid: wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The neighbor ids, their scores and the whole neighorhood graph (for visualization purposes)\n",
    "        \n",
    "    \"\"\"\n",
    "    log('[concept_embedding started]\\twid = %s, direction = %s', wid, direction)\n",
    "\n",
    "    if direction == DIR_IN or direction==DIR_OUT:\n",
    "        em = _concept_embedding_io(wid, direction)\n",
    "    if direction == DIR_BOTH:\n",
    "        em = _concept_embedding_both(wid, direction)\n",
    "    log('[concept_embedding]\\tfinished')\n",
    "    return em\n",
    "    \n",
    "def _concept_embedding_io(wid, direction):\n",
    "    wid = resolveredir(wid)\n",
    "    cached_em = checkcache(wid, direction);\n",
    "    if cached_em is not None:\n",
    "        return cached_em;\n",
    "\n",
    "    (ids, links) = getneighbors(wid, direction);\n",
    "    if ids:\n",
    "        scores = moler_pagerank_sparse_power(create_csr(links), reverse=True)\n",
    "        em = pd.Series(scores, index=ids) \n",
    "    else:\n",
    "        em = pd.Series([], index=[])  \n",
    "    cachescores(wid, em, direction);\n",
    "    return em\n",
    "            \n",
    "\n",
    "def _concept_embedding_both(wid, direction):            \n",
    "        in_em = _concept_embedding_io(wid, DIR_IN);\n",
    "        out_em = _concept_embedding_io(wid, DIR_OUT )\n",
    "        if (in_em is None) or (out_em is None):\n",
    "            return None;\n",
    "        return in_em.add(out_em, fill_value=0)/2\n",
    "\n",
    "def conceptrep_rvs(wid, direction, get_titles=True, cutoff=None):\n",
    "    \"\"\" Finds a representation for a concept\n",
    "    \n",
    "        Concept Representation is a vector of concepts with their score\n",
    "    Arg:\n",
    "        wid: Wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        the vecotr of ids, their titles and theirs scores. It also returns the\n",
    "        graph for visualization purposes. \n",
    "    \"\"\"\n",
    "    \n",
    "    log('[conceptrep started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    em=concept_embedding(wid, direction);    \n",
    "    if em.empty:\n",
    "        return em;\n",
    "    \n",
    "    \n",
    "    #ids = em.keys();\n",
    "    \n",
    "    if cutoff is not None:\n",
    "        em = em.sort_values(ascending=False)\n",
    "        em = em[:cutoff]\n",
    "    if get_titles:\n",
    "        em = pd.Series(zip(ids2title(em.index.tolist()), em.values.tolist()), index=em.index)\n",
    "    log ('[conceptrep]\\tfinished')\n",
    "    return em\n",
    "\n",
    "def gensim_loadmodel(model_path):\n",
    "    \"\"\" Loads the word2vec model \n",
    "    Arg:\n",
    "        model_path: path to the model\n",
    "    \"\"\"\n",
    "    global _word2vec_model\n",
    "    log('[getsim_word2vec]\\tloading: %s', model_path)\n",
    "    _word2vec_model = gensim.models.Word2Vec.load(model_path)                \n",
    "    log('[getsim_word2vec]\\loaded')\n",
    "    return _word2vec_model\n",
    "    \n",
    "def getentity2vector(wid):\n",
    "    if (wid is None) or (wid not in _word2vec_model.vocab):\n",
    "        return  pd.Series(sp.zeros(_word2vec_model.vector_size))\n",
    "    return pd.Series(_word2vec_model[wid])\n",
    "\n",
    "def getword2vector(word):\n",
    "    if word not in _word2vec_model.vocab:\n",
    "        return  pd.Series(sp.zeros(_word2vec_model.vector_size))\n",
    "    return pd.Series(_word2vec_model[word])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Semantic Relatedness\n",
    "The idea is get the neighborhood graph for each concept and calculating the similarity by embedding the graph into a vector and then perforiming cosine similarity. \n",
    "\n",
    "The process can be illustrated like this:\n",
    "    ![](../resrc/alg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile calcsim.py \n",
    "\"\"\"Calculating Relatedness.\"\"\"\n",
    "# uncomment\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from embedding import *\n",
    "#from collections import defaultdict\n",
    "import json\n",
    "import math\n",
    "from scipy import stats\n",
    "import requests\n",
    "from config import *\n",
    "\n",
    "#from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "#constants\n",
    "\n",
    "\n",
    "def _unify_ids_scores(*id_sc_tuple):\n",
    "    uids, id2in = e2i(*(ids for ids, _ in id_sc_tuple));\n",
    "    \n",
    "    uscs=tuple();            \n",
    "    for ids,scs in id_sc_tuple:\n",
    "        scs_u=sp.zeros(len(id2in))\n",
    "        scs_u[[id2in[wid] for wid in ids]] = scs;            \n",
    "        uscs += (scs_u,)                \n",
    "    return uids, uscs       \n",
    "\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "    q='+text:(%s)'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = requests.get(qstr, params=params)\n",
    "    D = r.json()['response']\n",
    "    return D['numFound']\n",
    "\n",
    "def getsim_ngd(term1,term2):\n",
    "    \"\"\" Calculates Normalized Google Distance (ngd) similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    \n",
    "    f1=get_solr_count('\"%s\"'%(term1,))\n",
    "    \n",
    "    f2=get_solr_count('\"%s\"'%(term2,))\n",
    "    f12=get_solr_count('\"%s\" AND \"%s\"'%(term1, term2))\n",
    "    \n",
    "    if (f1==0) or (f2==0) or (f12==0):\n",
    "        return 0;\n",
    "    dist = (sp.log(max(f1,f2))-sp.log(f12))/(sp.log(WIKI_SIZE)-sp.log(min(f1,f2)));\n",
    "    sim = 1-dist if dist <=1 else 0\n",
    "    return sim\n",
    "        \n",
    "def getsim_word2vec(id1, id2):\n",
    "    \"\"\" Calculates wor2vec similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    model  = getword2vec_model()\n",
    "    if model is None:\n",
    "        log('[getsim_word2vec]\\tmodel not loaded')\n",
    "        raise Exception('model not loaded, try gensim_loadmodel()')\n",
    "        \n",
    "    if id1 not in model.vocab:\n",
    "        #print '%s,%s skipped, %s not in vocab ' % (id1, id2, id1)\n",
    "        return 0\n",
    "    if id2 not in model.vocab:\n",
    "        #print '%s,%s skipped, %s not in vocab ' % (id1, id2, id2)\n",
    "        return 0\n",
    "    return model.similarity(id1, id2)\n",
    "\n",
    "\n",
    "def getsim_wlm(id1, id2):\n",
    "    \"\"\" Calculates wlm (ngd) similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    f12=len(in1.intersection(in2))\n",
    "    if (f1==0) or (f2==0) or (f12==0):\n",
    "        return 0;\n",
    "    dist = (sp.log(max(f1,f2))-sp.log(f12))/(sp.log(WIKI_SIZE)-sp.log(min(f1,f2)));\n",
    "    sim = 1-dist if dist <=1 else 0\n",
    "    return sim\n",
    "\n",
    "def getsim_cocit(id1, id2):\n",
    "    \"\"\" Calculates co-citation similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getsim_coup(id1, id2):\n",
    "    \"\"\" Calculates coupler similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    in2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "def getsim_ams(id1, id2):\n",
    "    \"\"\" Calculates amlser similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    out1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    link1 = in1.union(out1)\n",
    "    \n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    out2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    link2 = in2.union(out2)\n",
    "    \n",
    "    f1 = len(link1)\n",
    "    f2 = len(link2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(link1.intersection(link2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getsim_emb(id1,id2, direction):\n",
    "    \"\"\" Calculates the similarity between two concepts\n",
    "    Arg:\n",
    "        id1, id2: the two concepts\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The similarity score\n",
    "    \"\"\"\n",
    "    em1 = concept_embedding(id1, direction);\n",
    "    em2 = concept_embedding(id2, direction);\n",
    "    if em1.empty or em2.empty:\n",
    "        return 0;\n",
    "    \n",
    "    em1, em2 = em1.align(em2, fill_value=0)\n",
    "#     print em1\n",
    "#     print em2\n",
    "    return 1-sp.spatial.distance.cosine(em1.values,em2.values);\n",
    "\n",
    "def encode_entity(term, method,get_id=True):    \n",
    "    if method in {'ngd'}:\n",
    "        return term\n",
    "    if get_id:\n",
    "        term = title2id(term)\n",
    "    if term is None:\n",
    "        return None\n",
    "    if method in {'rvspagerank','wlm','cocit','coup','ams'} :\n",
    "        return term\n",
    "    if 'word2vec_id' in method:\n",
    "        term = 'id_' + str(term)\n",
    "        return term\n",
    "    if 'word2vec':\n",
    "        term = str(term)\n",
    "        return term\n",
    "    \n",
    "    return term\n",
    "\n",
    "def getsim(id1,id2, method='rvspagerank', direction=DIR_BOTH, sim_method=None):\n",
    "    \"\"\" Calculates well-known similarity metrics between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "        method:\n",
    "            wlm: Wikipedia-Miner method\n",
    "            cocit: cocitation\n",
    "            coup: coupling\n",
    "            ams: amsler\n",
    "            rvspagerank: ebedding based similarity (in our case, \n",
    "                 reversed-page rank method)\n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    log('[getsim started]\\method = %s, direction = %s, id1=%s, id2=%s', method, direction, id1, id2)\n",
    "    \n",
    "    if method=='rvspagerank':\n",
    "        sim = getsim_emb(id1,id2, direction)\n",
    "    elif method=='wlm':\n",
    "        sim = getsim_wlm(id1,id2)\n",
    "    elif method=='cocit':\n",
    "        sim = getsim_cocit(id1,id2)\n",
    "    elif method=='coup':\n",
    "        sim = getsim_coup(id1,id2)\n",
    "    elif method=='ams':\n",
    "        sim = getsim_ams(id1,id2)\n",
    "    elif 'word2vec' in  method:\n",
    "        sim = getsim_word2vec(id1, id2)\n",
    "    elif 'ngd' in  method:\n",
    "        sim = getsim_ngd(id1, id2)\n",
    "    elif sim_method is not None:    \n",
    "        sim = sim_method(id1,id2)\n",
    "    else:\n",
    "        sim=None\n",
    "    log('[getsim]\\tfinished')\n",
    "    return sim\n",
    "\n",
    "    \n",
    "def getsim_file(infilename, outfilename, method='rvspagerank', direction=DIR_BOTH, sim_method=None):\n",
    "    \"\"\" Batched (file) similarity.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "                        \n",
    "    Returns:\n",
    "        vector of scores, and Spearmans's correlation if goldstandard is given\n",
    "    \"\"\"\n",
    "    log('[getsim_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename);\n",
    "    gs=[];\n",
    "    scores=[];\n",
    "    spcorr=None;\n",
    "    for row in dsdata.itertuples():   \n",
    "        log('processing %s, %s', row[1], row[2])\n",
    "        if (row[1]=='null') or (row[2]=='null'):\n",
    "            continue;\n",
    "        if len(row)>3: \n",
    "            gs.append(row[3]);\n",
    "            \n",
    "        term1 = encode_entity(row[1], method, get_id=True)\n",
    "        term2 = encode_entity(row[2], method, get_id=True)\n",
    "            \n",
    "        if (term1 is None) or (term2 is None):\n",
    "            sim=0;\n",
    "        else:\n",
    "            sim=getsim(term1, term2, method, direction, sim_method);\n",
    "        outfile.write(\"\\t\".join([str(row[1]), str(row[2]), str(sim)])+'\\n')\n",
    "        scores.append(sim)\n",
    "    outfile.close();\n",
    "    if gs:\n",
    "        spcorr = sp.stats.spearmanr(scores, gs);\n",
    "    log('[getsim_file]\\tfinished')\n",
    "    return scores, spcorr\n",
    "\n",
    "    \n",
    "\n",
    "def getembed_file(infilename, outfilename, direction, get_titles=False, cutoff=None):\n",
    "    \"\"\" Batched (file) concept representation.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    log('[getembed_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename, usecols=[0]);\n",
    "    scores=[];\n",
    "    for row in dsdata.itertuples():        \n",
    "        wid = encode_entity(row[1], method='rvspagerank', get_id=True)\n",
    "        if wid is None:\n",
    "            em=pd.Series();\n",
    "        else:\n",
    "            em=conceptrep(wid, method='rvspagerank', direction = direction, \n",
    "                          get_titles = get_titles, cutoff=cutoff)\n",
    "        outfile.write(row[1]+\"\\t\"+em.to_json()+\"\\n\")\n",
    "    outfile.close();\n",
    "    log('[getembed_file]\\tfinished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('page_title: ', 'Abortion')\n",
      "('id: ', 765L)\n",
      "synonym ring: ('Abortion', 'Termination_of_pregnancy', 'Abortions', 'Induced_abortion', 'Abortionist')\n",
      " \n",
      "Concept Representation:  {\"765\":[\"Abortion\",0.0292042238],\"30044455\":[\"Therapeutic_abortion\",0.0077943629],\"492759\":[\"Late_termination_of_pregnancy\",0.0052510652],\"4289811\":[\"History_of_abortion\",0.0052268208],\"323793\":[\"Abortifacient\",0.0048862504]}\n",
      "\n",
      "\n",
      "\n",
      "('page_title: ', 'Miscarriage')\n",
      "('id: ', 144147L)\n",
      "synonym ring: ('Miscarriage', 'Miscarreage', 'Threatened_abortion', 'Inevitable_abortion', 'Incomplete_abortion')\n",
      " \n",
      "Concept Representation: {\"144147\":[\"Miscarriage\",0.0389120779],\"1771587\":[\"Pregnancy\",0.0136302541],\"429542\":[\"Preterm_birth\",0.0123620717],\"2300688\":[\"Complications_of_pregnancy\",0.0115965307],\"54309\":[\"Ectopic_pregnancy\",0.0114757366]}\n",
      "\n",
      "('similarity', 0.15728759037488615)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%aimport calcsim\n",
    "\n",
    "%aimport wikipedia\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from calcsim import *   # uncomment\n",
    "# Examples\n",
    "reopen()\n",
    "direction = DIR_OUT\n",
    "\n",
    "page_title1 = 'Abortion' \n",
    "print ('page_title: ', page_title1)\n",
    "\n",
    "page_id1 = title2id(page_title1)\n",
    "print (\"id: \", page_id1)\n",
    "\n",
    "sr1 = synonymring_titles(page_id1)\n",
    "print (\"synonym ring: %s\\n \" % str(sr1[:5]))\n",
    "\n",
    "rep1=conceptrep(page_id1, method='rvspagerank', direction = direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation:  %s\\n\" % rep1.to_json())\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "page_title2 = 'Miscarriage' \n",
    "print ('page_title: ', page_title2)\n",
    "\n",
    "page_id2 = title2id(page_title2)\n",
    "print (\"id: \", page_id2)\n",
    "\n",
    "sr2 = synonymring_titles(page_id2)\n",
    "print (\"synonym ring: %s\\n \" % str(sr2[:5]))\n",
    "\n",
    "rep2=conceptrep(page_id2, method='rvspagerank', direction = direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation: %s\\n\" % rep2.to_json())\n",
    "\n",
    "\n",
    "\n",
    "sim = getsim(page_id1, page_id2,'rvspagerank',DIR_IN)\n",
    "print (\"similarity\", sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating All The Embeddngs\n",
    "### Note: This step might take several days\n",
    "This step can be safely skipped and let the caching mechansism happens gradually over time, but if you have some heavy task, it is worth to invest some time and calculate the embeddings off-line\n",
    "\n",
    "### First. Get a list of the id pages\n",
    "```\n",
    "SELECT page_id\n",
    "INTO OUTFILE '~/backup/wikipedia/20160305/edited/enwiki-20160305-page.dumped.ssv'\n",
    "FROM page\n",
    "where page_namespace=0 and page_is_redirect=0 ;\n",
    "```\n",
    "### Second. Starting the precalculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preembed.py \n",
    "\"\"\"Pre calculation of the embeddings\"\"\"\n",
    "\n",
    "from config import *\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport calcsim\n",
    "from calcsim import *\n",
    "direction = DIR_IN;\n",
    "dirstr = graphtype(direction)\n",
    "#wid_fname  = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-page.dumped.ssv')\n",
    "wid_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.dead_2.ssv')\n",
    "\n",
    "done_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.done.ssv')\n",
    "dead_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.dead_3.ssv')\n",
    "rewrite = True\n",
    "lastwid = \"\"\n",
    "if os.path.exists(done_fname):\n",
    "    with open(done_fname) as done_f:\n",
    "        for lastwid in done_f:\n",
    "            pass\n",
    "        if lastwid is not None:\n",
    "            lastwid = lastwid.strip() \n",
    "            \n",
    "\n",
    "wid_f = open(wid_fname)\n",
    "done_f = open(done_fname, 'a')\n",
    "dead_f = open(dead_fname, 'a')\n",
    "    \n",
    "if lastwid:\n",
    "    for line in wid_f:\n",
    "        if line.strip() == lastwid:\n",
    "            break\n",
    "    print \"Continuing from \", lastwid\n",
    "else: \n",
    "    print \"Fresh start\"\n",
    "    \n",
    "for line in wid_f:\n",
    "    wid = line.strip().split('\\t')[0]\n",
    "    if rewrite:\n",
    "        deletefromcache(wid, direction)\n",
    "    em = concept_embedding(wid, direction)\n",
    "    if em.empty:\n",
    "        count = str(len(getlinkedpages(wid, direction)))\n",
    "        dead_f.write(wid+'\\t'+id2title(wid)+'\\t'+count+'\\n')\n",
    "    done_f.write(wid+'\\n')\n",
    "wid_f.close()\n",
    "done_f.close()\n",
    "dead_f.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing The Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from calcsim import *\n",
    "\n",
    "import json\n",
    "from IPython.display import Javascript\n",
    "\n",
    "cre1 = conceptrep(title2id('Tehran'), method='rvspagerank', direction = DIR_OUT, get_titles=True, cutoff=5);\n",
    "cre2 = conceptrep(title2id('Sanandaj'), method='rvspagerank', direction = DIR_OUT, get_titles=True, cutoff=5);\n",
    "\n",
    "\n",
    "#runs arbitrary javascript, client-side\n",
    "Javascript(\"\"\"\n",
    "           window.vizObj1={};window.vizObj2={};\n",
    "           \"\"\".format(cre1.to_json(), cre2.to_json()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/d3',\n",
    "        d3_cloud:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/d3.layout.cloud',\n",
    "        simple_draw:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/simpledraw'\n",
    "\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "function createWords(cp){\n",
    "\n",
    "    var titles=[];\n",
    "    var scores=[];\n",
    "\n",
    "    for (var key in cp){ \n",
    "        if (cp.hasOwnProperty(key)) {\n",
    "            titles.push(cp[key][0])\n",
    "            scores.push(cp[key][1])\n",
    "        }\n",
    "    }\n",
    "    var sum = scores.reduce(function(a, b) {return a + b;});\n",
    "    var min = Math.min.apply(null, scores)\n",
    "    var max = Math.max.apply(null, scores)\n",
    "    \n",
    "    scores=scores.map(function(a){return (a/sum)*90+20});\n",
    "    var words=[];\n",
    "    for (var i = 0; i<titles.length; i++) {\n",
    "        words.push({\"text\":titles[i], \"size\": scores[i]})\n",
    "    }\n",
    "    return words;\n",
    "}\n",
    "\n",
    "var words1=createWords(window.vizObj1);\n",
    "//element.text(JSON.stringify(words1));\n",
    "var words2=createWords(window.vizObj2);\n",
    "require(['d3','d3_cloud', 'simple_draw'], function(d3,d3_cloud, simple_draw){\n",
    "    $(\"#chart1\").remove();\n",
    "    element.append(\"<div id='chart1' style='width:49%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words1, chart1);\n",
    "    \n",
    "    $(\"#chart2\").remove();\n",
    "    element.append(\"<div id='chart2' style='width:49%; margin-left:2%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words2, '#chart2');    \n",
    "    \n",
    "});    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
